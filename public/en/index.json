[{"body":"What is RPC? Most machines on internet communicate with each other via TCP/IP. However, TCP/IP only guarantees reliable data transmissions. We need to abstract more to build services:\n What is the format of data transmission? Different machines and networks may have different byte-orders, directly sending in-memory data is not suitable. Fields in the data are added, modified or removed gradually, how do newer services talk with older services? Can TCP connection be reused for multiple requests to reduce overhead? Can multiple requests be sent through one TCP connection simultaneously? How to talk with a cluster with many machines? What should I do when the connection is broken? What if the server does not respond? \u0026hellip;  RPC addresses the above issues by abstracting network communications as \u0026ldquo;clients accessing functions on servers\u0026rdquo;: client sends a request to server, wait until server receives -\u0026gt; processes -\u0026gt; responds to the request, then do actions according to the result. Let\u0026rsquo;s see how the issues are solved.\n RPC needs serialization which is done by protobuf pretty well. Users fill requests in format of protobuf::Message, do RPC, and fetch results from responses in protobuf::Message. protobuf has good forward and backward compatibility for users to change fields and build services incrementally. For http services, json is used for serialization extensively. Establishment and re-using of connections is transparent to users, but users can make choices like different connection types: short, pooled, single. Machines are discovered by a Naming Service, which can be implemented by DNS, ZooKeeper or etcd. Inside Baidu, we use BNS (Baidu Naming Service). brpc provides \u0026ldquo;list://\u0026rdquo; and \u0026ldquo;file://\u0026quot; as well. Users specify load balancing algorithms to choose one machine for each request from all machines, including: round-robin, randomized, consistent-hashing(murmurhash3 or md5) and locality-aware. RPC retries when the connection is broken. When server does not respond within the given time, client fails with a timeout error.  Where can I use RPC? Almost all network communications.\nRPC can\u0026rsquo;t do everything surely, otherwise we don\u0026rsquo;t need the layer of TCP/IP. But in most network communications, RPC meets requirements and isolates the underlying details.\nCommon doubts on RPC:\n My data is binary and large, using protobuf will be slow. First, this is possibly a wrong feeling and you will have to test it and prove it with profilers. Second, many protocols support carrying binary data along with protobuf requests and bypass the serialization. I\u0026rsquo;m sending streaming data which can\u0026rsquo;t be processed by RPC. Actually many protocols in RPC can handle streaming data, including ProgressiveReader in http, streams in h2, streaming rpc, and RTMP which is a specialized streaming protocol. I don\u0026rsquo;t need replies. With some inductions, we know that in your scenario requests can be dropped at any stage because the client is always unaware of the situation. Are you really sure this is acceptable? Even if you don\u0026rsquo;t need the reply, we recommend sending back small-sized replies, which are unlikely to be performance bottlenecks and will probably provide valuable clues when debugging complex bugs.  What is ? An industrial-grade RPC framework used throughout Baidu, with 1,000,000+ instances(not counting clients) and thousands kinds of services, called \u0026ldquo;baidu-rpc\u0026rdquo; inside Baidu. Only C++ implementation is opensourced right now.\nYou can use it to:\n Build a server that can talk in multiple protocols (on same port), or access all sorts of services  restful http/https, h2/gRPC. using http/h2 in brpc is much more friendly than libcurl. Access protobuf-based protocols with HTTP/h2+json, probably from another language. redis and memcached, thread-safe, more friendly and performant than the official clients rtmp/flv/hls, for building streaming services. hadoop_rpc (may be opensourced) rdma support (will be opensourced) thrift support, thread-safe, more friendly and performant than the official clients. all sorts of protocols used in Baidu: baidu_std, streaming_rpc, hulu_pbrpc, sofa_pbrpc, nova_pbrpc, public_pbrpc, ubrpc, and nshead-based ones. Build HA distributed services using an industrial-grade implementation of RAFT consensus algorithm which is opensourced at braft   Servers can handle requests synchronously or asynchronously. Clients can access servers synchronously, asynchronously, semi-synchronously, or use combo channels to simplify sharded or parallel accesses declaratively. Debug services via http, and run cpu, heap and contention profilers. Get better latency and throughput. Extend brpc with the protocols used in your organization quickly, or customize components, including naming services (dns, zk, etcd), load balancers (rr, random, consistent hashing)  Advantages of brpc More friendly API Only 3 (major) user headers: Server, Channel, Controller, corresponding to server-side, client-side and parameter-set respectively. You don\u0026rsquo;t have to worry about \u0026ldquo;How to initialize XXXManager\u0026rdquo;, \u0026ldquo;How to layer all these components together\u0026rdquo;, \u0026ldquo;What\u0026rsquo;s the relationship between XXXController and XXXContext\u0026rdquo;. All you need to do is simple:\n  Build service? include brpc/server.h and follow the comments or examples.\n  Access service? include brpc/channel.h and follow the comments or examples.\n  Tweak parameters? Checkout brpc/controller.h. Note that the class is shared by server and channel. Methods are separated into 3 parts: client-side, server-side and both-side.\n  We tried to make simple things simple. Take naming service as an example. In older RPC implementations you may need to copy a pile of obscure code to make it work, however, in brpc accessing BNS is expressed as Init(\u0026quot;bns://node-name\u0026quot;, ...), DNS is Init(\u0026quot;http://domain-name\u0026quot;, ...) and local machine list is Init(\u0026quot;file:///home/work/server.list\u0026quot;, ...). Without any explanation, you know what it means.\nMake services more reliable brpc is extensively used in Baidu:\n map-reduce service \u0026amp; table storages high-performance computing \u0026amp; model training all sorts of indexing \u0026amp; ranking servers ….  It\u0026rsquo;s been proven.\nbrpc pays special attentions to development and maintenance efficency, you can view internal status of servers in web browser or with curl, analyze cpu hotspots, heap allocations and lock contentions of online services, measure stats by bvar which is viewable in /vars.\nBetter latency and throughput Although almost all RPC implementations claim that they\u0026rsquo;re \u0026ldquo;high-performant\u0026rdquo;, the numbers are probably just numbers. Being really high-performant in different scenarios is difficult. To unify communication infra inside Baidu, brpc goes much deeper at performance than other implementations.\n Reading and parsing requests from different clients is fully parallelized and users don\u0026rsquo;t need to distinguish between \u0026ldquo;IO-threads\u0026rdquo; and \u0026ldquo;Processing-threads\u0026rdquo;. Other implementations probably have \u0026ldquo;IO-threads\u0026rdquo; and \u0026ldquo;Processing-threads\u0026rdquo; and hash file descriptors(fd) into IO-threads. When a IO-thread handles one of its fds, other fds in the thread can\u0026rsquo;t be handled. If a message is large, other fds are significantly delayed. Although different IO-threads run in parallel, you won\u0026rsquo;t have many IO-threads since they don\u0026rsquo;t have too much to do generally except reading/parsing from fds. If you have 10 IO-threads, one fd may affect 10% of all fds, which is unacceptable to industrial online services (requiring 99.99% availability). The problem will be worse when fds are distributed unevenly accross IO-threads (unfortunately common), or the service is multi-tenancy (common in cloud services). In brpc, reading from different fds is parallelized and even processing different messages from one fd is parallelized as well. Parsing a large message does not block other messages from the same fd, not to mention other fds. More details can be found here. Writing into one fd and multiple fds is highly concurrent. When multiple threads write into the same fd (common for multiplexed connections), the first thread directly writes in-place and other threads submit their write requests in wait-free manner. One fd can be written into 5,000,000 16-byte messages per second by a couple of highly-contended threads. More details can be found here. Minimal locks. High-QPS services can utilize all CPU power on the machine. For example, creating bthreads for processing requests, setting up timeout, finding RPC contexts according to response, recording performance counters are all highly concurrent. Users see very few contentions (via contention profiler) caused by RPC framework even if the service runs at 500,000+ QPS. Server adjusts thread number according to load. Traditional implementations set number of threads according to latency to avoid limiting the throughput. brpc creates a new bthread for each request and ends the bthread when the request is done, which automatically adjusts thread number according to load.  Check benchmark for a comparison between brpc and other implementations.\n","excerpt":"What is RPC? Most machines on internet communicate with each other via TCP/IP. However, TCP/IP only …","ref":"https://brpc.incubator.apache.org/en/docs/overview/","title":"bRPC overview"},{"body":"To contribute to MOSN\u0026rsquo;s documentation, you need to:\n Create a GitHub account.  This documentation is published in compliance with Apache 2.0 license.\nHow to contribute You can contribute to MOSN\u0026rsquo;s documentation in the following three ways:\n To edit an existing topic, open the page with your browser, click Edit This Page on the upper-right side, edit the GitHub page that appears, and submit the modifications. To make general edits, follow the procedure in Add content. To review an existing pull request (PR), follow the procedure in the Review PRs.  PRs are immediately displayed on https://mosn.io after being merged.\nAdd content To add content, you need to create a repository branch and submit a PR to the mater repository from the branch. Perform the following steps:\n Access the MOSN repository at GitHub https://github.com/apache/incubator-brpc-website. Click Fork in the upper-right corner to fork a copy of the MOSN repository to your GitHub account. Clone your fork to your computer and make modifications as required. Upload the modifications to your fork repository when you are ready to submit them to us. Go to the index page of your fork repository and click New pull request to submit a PR.  Review PRs You can directly comment on a PR. To add detailed comments, perform the following steps:\n Add detailed comments to the PR. If possible, directly add comments to the corresponding files and file lines. Provide suggestions to the PR authors and contributors in the comments as appropriate. Publish and share your comments with the PR contributors. Merge the PR after publishing the comments and reaching an agreement with the contributors.  Preview PRs You can preview your PR online or run Hugo on your computer to access the MOSN website for real-time preview.\nOnline preview After you submit a PR, a series of check items are displayed on the corresponding PR page at GitHub. The deploy/netlify step generates the preview page on the MOSN official website. You can click Details to go to the preview page. A preview page is constructed each time you submit the same PR.\nThis temporary website ensures normal page display after the PR is merged.\nLocal preview In addition to online preview, you can also preview your PR with Hugo (the v0.55.5 extended version is recommended). You can run hugo server at the root directory of your code repository and then access http://localhost:1313 with your browser.\n","excerpt":"To contribute to MOSN\u0026rsquo;s documentation, you need to:\n Create a GitHub account.  This …","ref":"https://brpc.incubator.apache.org/en/docs/contribute/github/","title":"Documentation contribution guide"},{"body":"Modular Open Smart Network (MOSN) mainly comprises the following modules. It provides basic network proxy features, and cloud-native services such as xDS.\nxDS (UDPA) support MOSN supports cloud-native Universal Data Plane APIs (UDPAs) and fully dynamic configuration updates.\nxDS is a key concept proposed by Envoy. It represents a set of discovery services:\n CDS: Cluster Discovery Service EDS: Endpoint Discovery Service SDS: Secret Discovery Service RDS: Route Discovery Service LDS: Listener Discovery Service  Envoy configurations are dynamically updated through xDS requests. The Aggregated Discovery Service (ADS) determines the xDS update order through aggregation.\nBusiness support As an underlying high-performance security network proxy, MOSN supports a variety of business scenarios, such as remote procedure calls (RPCs), messaging, gateways.\nI/O model MOSN supports the following two I/O models:\n  Golang\u0026rsquo;s classic netpoll model: a goroutine-per-connection model applicable when the number of connections is not a bottleneck.\n  RawEpoll model: also known as the Reactor model, is an I/O multiplexing + non-blocking I/O model. The RawEpoll model is more suitable for scenarios with a large number of persistent connections at the access layer and gateways.\n  netpoll model The MOSN netpoll model is shown in the above figure. The number of goroutines is proportional to that of connections. A large number of connections indicates a large number of goroutines and high overheads, including:\n Stack memory overhead Read buffer overhead Runtime scheduling overhead  RawEpoll model The RawEpoll model is shown in the above figure. After a readable event is detected by epoll, a goroutine is allocated from the goroutine pool to process the event. The procedure is as follows:\n After a connection is established, MOSN registers a oneshot readable event listener with epoll. To avoid conflict with the runtime netpoll, no goroutine is allowed to call conn.read at this time. After a readable event is detected, MOSN selects a goroutine from the goroutine pool to handle the event. No subsequent readable event will be triggered for the file descriptor (FD) again because the oneshot mode is used. During request handling, the goroutine scheduling process is consistent with that of the classic netpool model. After the request handling is complete, the goroutine is returned to the goroutine pool, and the FD is added back to RawEpoll.  Goroutine model The MOSN goroutine model is shown in the following figure.\n One TCP connection corresponds to one read goroutine, for receiving packets and parsing protocols. One request corresponds to one worker goroutine, for handling business and executing the proxy and write logic.  In the conventional model, one TCP connection corresponds to two goroutines: read and write. We have replaced the separate write goroutine with a workerpool goroutine, to reduce scheduling latency and memory usage.\nCapability extension Protocol extension MOSN implements a protocol plugin mechanism by using a unified codec engine and core codec APIs, providing support for:\n SOFARPC HTTP1.x/HTTP2.0 Dubbo  NetworkFilter extension MOSN implements a NetworkFilter extension mechanism by using a NetworkFilter registration mechanism and unified packet read/write filter APIs, providing support for:\n TCP proxy Fault injection  StreamFilter extension MOSN implements a StreamFilter extension mechanism by using a StreamFilter registration mechanism and unified stream send/receive filter APIs, providing support for:\n Traffic mirroring RBAC authentication  TLS connections Based on tests, the Go-native TLS, after a lot of assembly optimization, has improved its performance to 80% of that of NGINX (OpenSSL). go-boring (which calls BoringSSL by using CGO) shows no advantage, because it is restricted by the performance of CGO. Therefore, we chose the Go-native TLS at last. We believe that the Go Runtime team will continue the optimization. We have some optimization plans, too.\nThe following figure shows the comparison between the test results of Go and NGINX.\n Go is not optimized much for RSA encryption, and go-boring (CGO) performs twice as good as Go in this regard. Go has been assembly-optimized for ECDSA P256, and it performs better than go-boring in this respect. Go performs 20 times as good as go-boring in terms of AES-GCM symmetric encryption. Go has also been assembly-optimized for hashing algorithms such as SHA and MD.  To ensure security compliance in financial scenarios, we also support development based on Chinese encryption algorithms, which is not supported by Go Runtime. Currently, the performance of Chinese encryption algorithms is only 50% of that of the international standard AES-GCM encryption algorithms. We have some plans to improve their performance. Please stay tuned.\nPerformance test results for Chinese encryption algorithms are shown in the following figure.\n","excerpt":"Modular Open Smart Network (MOSN) mainly comprises the following modules. It provides basic network …","ref":"https://brpc.incubator.apache.org/en/docs/concept/core-concept/","title":"Core concepts of MOSN"},{"body":"BUILD brpc prefers static linkages of deps, so that they don\u0026rsquo;t have to be installed on every machine running the app.\nbrpc depends on following packages:\n gflags: Extensively used to define global options. protobuf: Serializations of messages, interfaces of services. leveldb: Required by /rpcz to record RPCs for tracing.  Supported Environment  Ubuntu/LinuxMint/WSL Fedora/CentOS Linux with self-built deps MacOS  Ubuntu/LinuxMint/WSL Prepare deps Install common deps, gflags, protobuf, leveldb:\nsudo apt-get install -y git g++ make libssl-dev libgflags-dev libprotobuf-dev libprotoc-dev protobuf-compiler libleveldb-dev If you need to statically link leveldb:\nsudo apt-get install -y libsnappy-dev If you need to enable cpu/heap profilers in examples:\nsudo apt-get install -y libgoogle-perftools-dev If you need to run tests, install and compile libgtest-dev (which is not compiled yet):\nsudo apt-get install -y cmake libgtest-dev \u0026amp;\u0026amp; cd /usr/src/gtest \u0026amp;\u0026amp; sudo cmake . \u0026amp;\u0026amp; sudo make \u0026amp;\u0026amp; sudo mv libgtest* /usr/lib/ \u0026amp;\u0026amp; cd - The directory of gtest source code may be changed, try /usr/src/googletest/googletest if /usr/src/gtest is not there.\nCompile brpc with config_brpc.sh git clone brpc, cd into the repo and run\n$ sh config_brpc.sh --headers=/usr/include --libs=/usr/lib $ make To change compiler to clang, add --cxx=clang++ --cc=clang.\nTo not link debugging symbols, add --nodebugsymbols and compiled binaries will be much smaller.\nTo use brpc with glog, add --with-glog.\nTo enable thrift support, install thrift first and add --with-thrift.\nRun example\n$ cd example/echo_c++ $ make $ ./echo_server \u0026amp; $ ./echo_client Examples link brpc statically, if you need to link the shared version, make clean and LINK_SO=1 make\nRun tests\n$ cd test $ make $ sh run_tests.sh Compile brpc with cmake cmake -B build \u0026amp;\u0026amp; cmake --build build -j6 To help VSCode or Emacs(LSP) to understand code correctly, add -DCMAKE_EXPORT_COMPILE_COMMANDS=ON to generate compile_commands.json\nTo change compiler to clang, overwrite environment variable CC and CXX to clang and clang++ respectively.\nTo not link debugging symbols, remove build/CMakeCache.txt and cmake with -DWITH_DEBUG_SYMBOLS=OFF\nTo use brpc with glog, cmake with -DWITH_GLOG=ON.\nTo enable thrift support, install thrift first and cmake with -DWITH_THRIFT=ON.\nRun example with cmake\n$ cd example/echo_c++ $ cmake -B build \u0026amp;\u0026amp; cmake --build build -j4 $ ./echo_server \u0026amp; $ ./echo_client Examples link brpc statically, if you need to link the shared version, remove CMakeCache.txt and cmake with -DLINK_SO=ON\nRun tests\n$ mkdir build \u0026amp;\u0026amp; cd build \u0026amp;\u0026amp; cmake -DBUILD_UNIT_TESTS=ON .. \u0026amp;\u0026amp; make \u0026amp;\u0026amp; make test Fedora/CentOS Prepare deps CentOS needs to install EPEL generally otherwise many packages are not available by default.\nsudo yum install epel-release Install common deps:\nsudo yum install git gcc-c++ make openssl-devel Install gflags, protobuf, leveldb:\nsudo yum install gflags-devel protobuf-devel protobuf-compiler leveldb-devel If you need to enable cpu/heap profilers in examples:\nsudo yum install gperftools-devel If you need to run tests, install and compile gtest-devel (which is not compiled yet):\nsudo yum install gtest-devel Compile brpc with config_brpc.sh git clone brpc, cd into the repo and run\n$ sh config_brpc.sh --headers=/usr/include --libs=/usr/lib64 $ make To change compiler to clang, add --cxx=clang++ --cc=clang.\nTo not link debugging symbols, add --nodebugsymbols and compiled binaries will be much smaller.\nTo use brpc with glog, add --with-glog.\nTo enable thrift support, install thrift first and add --with-thrift.\nRun example\n$ cd example/echo_c++ $ make $ ./echo_server \u0026amp; $ ./echo_client Examples link brpc statically, if you need to link the shared version, make clean and LINK_SO=1 make\nRun tests\n$ cd test $ make $ sh run_tests.sh Compile brpc with cmake Same with here\nLinux with self-built deps Prepare deps brpc builds itself to both static and shared libs by default, so it needs static and shared libs of deps to be built as well.\nTake gflags as example, which does not build shared lib by default, you need to pass options to cmake to change the behavior:\n$ cmake . -DBUILD_SHARED_LIBS=1 -DBUILD_STATIC_LIBS=1 $ make Compile brpc Keep on with the gflags example, let ../gflags_dev be where gflags is cloned.\ngit clone brpc. cd into the repo and run\n$ sh config_brpc.sh --headers=\u0026#34;../gflags_dev /usr/include\u0026#34; --libs=\u0026#34;../gflags_dev /usr/lib64\u0026#34; $ make Here we pass multiple paths to --headers and --libs to make the script search for multiple places. You can also group all deps and brpc into one directory, then pass the directory to \u0026ndash;headers/\u0026ndash;libs which actually search all subdirectories recursively and will find necessary files.\nTo change compiler to clang, add --cxx=clang++ --cc=clang.\nTo not link debugging symbols, add --nodebugsymbols and compiled binaries will be much smaller.\nTo use brpc with glog, add --with-glog.\nTo enable thrift support, install thrift first and add --with-thrift.\n$ ls my_dev gflags_dev protobuf_dev leveldb_dev brpc_dev $ cd brpc_dev $ sh config_brpc.sh --headers=.. --libs=.. $ make Compile brpc with cmake Same with here\nMacOS Note: In the same running environment, the performance of the current Mac version is about 2.5 times worse than the Linux version. If your service is performance-critical, do not use MacOS as your production environment.\nPrepare deps Install common deps:\nbrew install openssl git gnu-getopt coreutils Install gflags, protobuf, leveldb:\nbrew install gflags protobuf leveldb If you need to enable cpu/heap profilers in examples:\nbrew install gperftools If you need to run tests, download and compile googletest (which is not compiled yet):\ngit clone https://github.com/google/googletest -b release-1.10.0 \u0026amp;\u0026amp; cd googletest/googletest \u0026amp;\u0026amp; mkdir build \u0026amp;\u0026amp; cd build \u0026amp;\u0026amp; cmake -DCMAKE_CXX_FLAGS=\u0026#34;-std=c++11\u0026#34; .. \u0026amp;\u0026amp; make After the compilation, copy include/ and lib/ into /usr/local/include and /usr/local/lib respectively to expose gtest to all apps\nCompile brpc with config_brpc.sh git clone brpc, cd into the repo and run\n$ sh config_brpc.sh --headers=/usr/local/include --libs=/usr/local/lib --cc=clang --cxx=clang++ $ make To not link debugging symbols, add --nodebugsymbols and compiled binaries will be much smaller.\nTo use brpc with glog, add --with-glog.\nTo enable thrift support, install thrift first and add --with-thrift.\nRun example\n$ cd example/echo_c++ $ make $ ./echo_server \u0026amp; $ ./echo_client Examples link brpc statically, if you need to link the shared version, make clean and LINK_SO=1 make\nRun tests\n$ cd test $ make $ sh run_tests.sh Compile brpc with cmake Same with here\nSupported deps GCC: 4.8-7.1 c++11 is turned on by default to remove dependencies on boost (atomic).\nThe over-aligned issues in GCC7 is suppressed temporarily now.\nUsing other versions of gcc may generate warnings, contact us to fix.\nAdding -D__const__= to cxxflags in your makefiles is a must to avoid errno issue in gcc4+.\nClang: 3.5-4.0 no known issues.\nglibc: 2.12-2.25 no known issues.\nprotobuf: 2.4+ Be compatible with pb 3.x and pb 2.x with the same file: Don\u0026rsquo;t use new types in proto3 and start the proto file with syntax=\u0026quot;proto2\u0026quot;; tools/add_syntax_equal_proto2_to_all.shcan add syntax=\u0026quot;proto2\u0026quot; to all proto files without it.\nArena in pb 3.x is not supported yet.\ngflags: 2.0-2.2.1 no known issues.\nopenssl: 0.97-1.1 required by https.\ntcmalloc: 1.7-2.5 brpc does not link tcmalloc by default. Users link tcmalloc on-demand.\nComparing to ptmalloc embedded in glibc, tcmalloc often improves performance. However different versions of tcmalloc may behave really differently. For example, tcmalloc 2.1 may make multi-threaded examples in brpc perform significantly worse(due to a spinlock in tcmalloc) than the one using tcmalloc 1.7 and 2.5. Even different minor versions may differ. When you program behave unexpectedly, remove tcmalloc or try another version.\nCode compiled with gcc 4.8.2 and linked to a tcmalloc compiled with earlier GCC may crash or deadlock before main(), E.g:\nWhen you meet the issue, compile tcmalloc with the same GCC.\nAnother common issue with tcmalloc is that it does not return memory to system as early as ptmalloc. So when there\u0026rsquo;s an invalid memory access, the program may not crash directly, instead it crashes at a unrelated place, or even not crash. When you program has weird memory issues, try removing tcmalloc.\nIf you want to use cpu profiler or heap profiler, do link libtcmalloc_and_profiler.a. These two profilers are based on tcmalloc.contention profiler does not require tcmalloc.\nWhen you remove tcmalloc, not only remove the linkage with tcmalloc but also the macro -DBRPC_ENABLE_CPU_PROFILER.\nglog: 3.3+ brpc implements a default logging utility which conflicts with glog. To replace this with glog, add \u0026ndash;with-glog to config_brpc.sh or add -DWITH_GLOG=ON to cmake.\nvalgrind: 3.8+ brpc detects valgrind automatically (and registers stacks of bthread). Older valgrind(say 3.2) is not supported.\nthrift: 0.9.3-0.11.0 no known issues.\nTrack instances We provide a program to help you to track and monitor all brpc instances. Just run trackme_server somewhere and launch need-to-be-tracked instances with -trackme_server=SERVER. The trackme_server will receive pings from instances periodically and print logs when it does. You can aggregate instance addresses from the log and call builtin services of the instances for further information.\n","excerpt":"BUILD brpc prefers static linkages of deps, so that they don\u0026rsquo;t have to be installed on every …","ref":"https://brpc.incubator.apache.org/en/docs/getting_started/","title":"Getting started"},{"body":"序言 在多核的前提下，性能和线程是紧密联系在一起的。线程间的跳转对高频IO操作的性能有决定性作用: 一次跳转意味着至少3-20微秒的延时，由于每个核心的L1 cache独立（我们的cpu L2 cache也是独立的），随之而来是大量的cache miss，一些变量的读取、写入延时会从纳秒级上升几百倍至微秒级: 等待cpu把对应的cacheline同步过来。有时这带来了一个出乎意料的结果，当每次的处理都很简短时，一个多线程程序未必比一个单线程程序更快。因为前者可能在每次付出了大的切换代价后只做了一点点“正事”，而后者在不停地做“正事”。不过单线程也是有代价的，它工作良好的前提是“正事”都很快，否则一旦某次变慢就使后续的所有“正事”都被延迟了。在一些处理时间普遍较短的程序中，使用（多个不相交的）单线程能最大程度地”做正事“，由于每个请求的处理时间确定，延时表现也很稳定，各种http server正是这样。但我们的检索服务要做的事情可就复杂多了，有大量的后端服务需要访问，广泛存在的长尾请求使每次处理的时间无法确定，排序策略也越来越复杂。如果还是使用（多个不相交的）单线程的话，一次难以预计的性能抖动，或是一个大请求可能导致后续一堆请求被延迟。\n为了避免请求之间相互影响，请求级的线程跳转是brpc必须付出的代价，我们能做的是使线程跳转最优化。不过，对服务的性能测试还不能很好地体现这点。测试中的处理往往极为简单，使得线程切换的影响空前巨大，通过控制多线程和单线程处理的比例，我们可以把一个测试服务的qps从100万到500万操纵自如（同机），这损伤了性能测试结果的可信度。要知道，真实的服务并不是在累加一个数字，或者echo一个字符串，一个qps几百万的echo程序没有指导意义。鉴于此，在发起性能测试一年后（15年底），在brpc又经历了1200多次改动后，我们需要review所有的测试，加强其中的线程因素，以获得对真实场景有明确意义的结果。具体来说:\n 请求不应等长，要有长尾。这能考察RPC能否让请求并发，否则一个慢请求会影响大量后续请求。 要有多级server的场景。server内用client访问下游server，这能考察server和client的综合表现。 要有一个client访问多个server的场景。这能考察负载均衡是否足够并发，真实场景中很少一个client只访问一个server。  我们希望这套测试场景对其他服务的性能测试有借鉴意义。\n测试目标 UB 百度在08年开发的RPC框架，在百度产品线广泛使用，已被brpc代替。UB的每个请求独占一个连接(连接池)，在大规模服务中每台机器都需要保持大量的连接，限制了其使用场景，像百度的分布式系统没有用UB。UB只支持nshead+mcpack协议，也没怎么考虑扩展性，所以增加新协议和新功能往往要调整大段代码，在实践中大部分人“知难而退”了。UB缺乏调试和运维接口，服务的运行状态对用户基本是黑盒，只能靠低效地打日志来追踪问题，服务出现问题时常要拉上维护者一起排查，效率很低。UB有多个变种:\n ubrpc: 百度在10年基于UB开发的RPC框架，用.idl文件(类似.proto)描述数据的schema，而不是手动打包。这个RPC有被使用，但不广泛。   nova_pbrpc: 百度网盟团队在12年基于UB开发的RPC框架，用protobuf代替mcpack作为序列化方法，协议是nshead + user\u0026rsquo;s protobuf。 public_pbrpc: 百度在13年初基于UB开发的RPC框架，用protobuf代替mcpack作为序列化方法，但协议与nova_pbrpc不同，大致是nshead + meta protobuf。meta protobuf中有个string字段包含user\u0026rsquo;s protobuf。由于用户数据要序列化两次，这个RPC的性能很差，没有被推广开来。  我们以在百度网盟团队广泛使用的nova_pbrpc为UB的代表。测试时其代码为r10500。早期的UB支持CPOOL和XPOOL，分别使用select和leader-follower模型，后来提供了EPOLL，使用epoll处理多路连接。鉴于产品线大都是用EPOLL模型，我们的UB配置也使用EPOLL。UB只支持连接池，结果用“ubrpc_mc\u0026ldquo;指代（mc代表\u0026quot;multiple connection\u0026rdquo;）。虽然这个名称不太准确（见上文对ubrpc的介绍），但在本文的语境下，请默认ubrpc = UB。\nhulu-pbrpc 百度在13年基于saber(kylin变种)和protobuf实现的RPC框架，hulu在多线程实现上有较多问题，已被brpc代替，测试时其代码为pbrpc_2-0-15-27959_PD_BL。hulu-pbrpc只支持单连接，结果用“hulu-pbrpc\u0026ldquo;指代。\nbrpc INF在2014年底开发至今的rpc产品，支持百度内所有协议（不限于protobuf），并第一次统一了百度主要分布式系统和业务线的RPC框架。测试时代码为r31906。brpc既支持单连接也支持连接池，前者的结果用\u0026rdquo;baidu-rpc\u0026ldquo;指代，后者用“baidu-rpc_mc\u0026ldquo;指代。\nsofa-pbrpc 百度大搜团队在13年基于boost::asio和protobuf实现的RPC框架，有多个版本，咨询相关同学后，确认ps/opensource下的和github上的较新，且会定期同步。故测试使用使用ps/opensource下的版本。测试时其代码为sofa-pbrpc_1-0-2_BRANCH。sofa-pbrpc只支持单连接，结果用“sofa-pbrpc”指代。\napache thrift thrift是由facebook最早在07年开发的序列化方法和rpc框架，包含独特的序列化格式和IDL，支持很多编程语言。开源后改名apache thrift，fb自己有一个fbthrift分支，我们使用的是apache thrift。测试时其代码为thrift_0-9-1-400_PD_BL。thrift的缺点是: 代码看似分层清晰，client和server选择很多，但没有一个足够通用，每个server实现都只能解决很小一块场景，每个client都线程不安全，实际使用很麻烦。由于thrift没有线程安全的client，所以每个线程中都得建立一个client，使用独立的连接。在测试中thrift其实是占了其他实现的便宜: 它的client不需要处理多线程问题。thrift的结果用\u0026rdquo;thrift_mc\u0026ldquo;指代。\ngRPC 由google开发的rpc框架，使用http/2和protobuf 3.0，测试时其代码为https://github.com/grpc/grpc/tree/release-0_11。gRPC并不是stubby，定位更像是为了推广http/2和protobuf 3.0，但鉴于很多人对它的表现很感兴趣，我们也（很麻烦地）把它加了进来。gRPC的结果用\u0026rdquo;grpc\u0026ldquo;指代。\n测试方法 如序言中解释的那样，性能数字有巨大的调整空间。这里的关键在于，我们对RPC的底线要求是什么，脱离了这个底线，测试中的表现就严重偏离真实环境中的了。\n这个底线我们认为是RPC必须能处理长尾。\n在百度的环境中，这是句大白话，哪个产品线，哪个系统没有长尾呢？作为承载大部分服务的RPC框架自然得处理好长尾，减少长尾对正常请求的影响。但在实现层面，这个问题对设计的影响太大了。如果测试中没有长尾，那么RPC实现就可以假设每个请求都差不多快，这时候最优的方法是用多个线程独立地处理请求。由于没有上下文切换和cache一致性同步，程序的性能会显著高于多个线程协作时的表现。\n比如简单的echo程序，处理一个请求只需要200-300纳秒，单个线程可以达到300-500万的吞吐。但如果多个线程协作，即使在极其流畅的系统中，也要付出3-5微秒的上下文切换代价和1微秒的cache同步代价，这还没有考虑多个线程间的其他互斥逻辑，一般来说单个线程的吞吐很难超过10万，即使24核全部用满，吞吐也只有240万，不及一个线程。这正是以http server为典型的服务选用单线程模型的原因（多个线程独立运行eventloop）: 大部分http请求的处理时间是可预测的，对下游的访问也不会有任何阻塞代码。这个模型可以最大化cpu利用率，同时提供可接受的延时。\n多线程付出这么大的代价是为了隔离请求间的影响。一个计算复杂或索性阻塞的过程不会影响到其他请求，1%的长尾最终只会影响到1%的性能。而多个独立的线程是保证不了这点的，一个请求进入了一个线程就等于“定了终生”，如果前面的请求慢了一下，那也只能跟着慢了。1%的长尾会影响远超1%的请求，最终表现不佳。换句话说，乍看上去多线程模型“慢”了，但在真实应用中反而会获得更好的综合性能。\n延时能精确地体现出长尾的干扰作用，如果普通请求的延时没有被长尾请求干扰，就说明RPC成功地隔离了请求。而QPS无法体现这点，只要CPU都在忙，即使一个正常请求进入了挤满长尾的队列而被严重延迟，最终的QPS也变化不大。为了测量长尾的干扰作用，我们在涉及到延时的测试中都增加了1%的长尾请求。\n开始测试 环境 性能测试使用的机器配置为:\n 单机1: CPU开超线程24核，E5-2620 @ 2.00GHz；64GB内存；OS linux 2.6.32_1-15-0-0 多机1（15台+8台）: CPU均未开超线程12核，其中15台的CPU为E5-2420 @ 1.90GHz.，64GB内存，千兆网卡，无法开启多队列。其余8台为E5-2620 2.0GHz，千兆网卡，绑定多队列到前8个核。这些长期测试机器比较杂，跨了多个机房，测试中延时在1ms以上的就是这批机器。 多机2（30台）: CPU未开超线程12核，E5-2620 v3 @ 2.40GHz.；96GB内存；OS linux 2.6.32_1-17-0-0；万兆网卡，绑定多队列到前8个核。这是临时借用的新机器，配置非常好，都在广州机房，延时非常短，测试中延时在几百微秒的就是这批机器。  下面所有的曲线图是使用brpc开发的dashboard程序绘制的，去掉路径后可以看到和所有brpc server一样的内置服务。\n配置 如无特殊说明，所有测试中的配置只是数量差异（线程数，请求大小，client个数etc），而不是模型差异。我们确保用户看到的qps和延时是同一个场景的不同维度，而不是无法统一的两个场景。\n所有RPC server都配置了24个工作线程，这些线程一般运行用户的处理逻辑。关于每种RPC的特殊说明:\n UB: 配置了12个reactor线程，使用EPOOL模型。连接池限制数配置为线程个数（24） hulu-pbrpc: 额外配置了12个IO线程。这些线程会处理fd读取，请求解析等任务。hulu有个“共享队列“的配置项，默认不打开，作用是把fd静态散列到多个线程中，由于线程间不再争抢，hulu的qps会显著提高，但会明显地被长尾影响（原因见测试方法）。考虑到大部分使用者并不会去改配置，我们也选择不打开。 thrift: 额外配置了12个IO线程。这些线程会处理fd读取，请求解析等任务。thrift的client不支持多线程，每个线程得使用独立的client，连接也都是分开的。 sofa-pbrpc: 按照sofa同学的要求，把io_service_pool_size配置为24，work_thread_num配置为1。大概含义是使用独立的24组线程池，每组1个worker thread。和hulu不打开“共享队列”时类似，这个配置会显著提高sofa-pbrpc的QPS，但同时使它失去了处理长尾的能力。如果你在真实产品中使用，我们不建议这个配置。（而应该用io_service_pool_size=1, work_thread_num=24) brpc: 尽管brpc的client运行在bthread中时会获得10%~20%的QPS提升和更低的延时，但测试中的client都运行统一的pthread中。  所有的RPC client都以多个线程同步方式发送，这种方法最接近于真实系统中的情况，在考察QPS时也兼顾了延时因素。\n一种流行的方案是client不停地往连接中写入数据看server表现，这个方法的弊端在于: server一下子能读出大量请求，不同RPC的比拼变成了“for循环执行用户代码”的比拼，而不是分发请求的效率。在真实系统中server很少能同时读到超过4个请求。这个方法也完全放弃了延时，client其实是让server陷入了雪崩时才会进入的状态，所有请求都因大量排队而超时了。\n同机单client→单server在不同请求下的QPS（越高越好） 本测试运行在单机1上。图中的数值均为用户数据的字节数，实际的请求尺寸还要包括协议头，一般会增加40字节左右。\n（X轴是用户数据的字节数，Y轴是对应的QPS）\n以_mc结尾的曲线代表client和server保持多个连接（线程数个），在本测试中会有更好的表现。\n分析\n brpc: 当请求包小于16KB时，单连接下的吞吐超过了多连接的ubrpc_mc和thrift_mc，随着请求包变大，内核对单个连接的写入速度成为瓶颈。而多连接下的brpc则达到了测试中最高的2.3GB/s。注意: 虽然使用连接池的brpc在发送大包时吞吐更高，但也会耗费更多的CPU（UB和thrift也是这样）。下图中的单连接brpc已经可以提供800多兆的吞吐，足以打满万兆网卡，而使用的CPU可能只有多链接下的1/2(写出过程是wait-free的)，真实系统中请优先使用单链接。 thrift: 初期明显低于brpc，随着包变大超过了单连接的brpc。 UB:和thrift类似的曲线，但平均要低4-5万QPS，在32K包时超过了单连接的brpc。整个过程中QPS几乎没变过。 gRPC: 初期几乎与UB平行，但低1万左右，超过8K开始下降。 hulu-pbrpc和sofa-pbrpc: 512字节前高于UB和gRPC，但之后就急转直下，相继垫底。这个趋势是写不够并发的迹象。  同机单client→单server在不同线程数下的QPS（越高越好） 本测试运行在单机1上。\n（X轴是线程数，Y轴是对应的QPS）\n分析\nbrpc: 随着发送线程增加，QPS在快速增加，有很好的多线程扩展性。\nUB和thrift: 8个线程下高于brpc，但超过8个线程后被brpc迅速超过，thrift继续“平移”，UB出现了明显下降。\ngRPC，hulu-pbrpc，sofa-pbrpc: 几乎重合，256个线程时相比1个线程时只有1倍的提升，多线程扩展性不佳。\n同机单client→单server在固定QPS下的延时CDF（越左越好，越直越好） 本测试运行在单机1上。考虑到不同RPC的处理能力，我们选择了一个较低、在不少系统中会达到的的QPS: 1万。\n本测试中有1%的长尾请求耗时5毫秒，长尾请求的延时不计入结果，因为我们考察的是普通请求是否被及时处理了。\n（X轴是延时（微秒），Y轴是小于X轴延时的请求比例）\n分析\n brpc: 平均延时短，几乎没有被长尾影响。 UB和thrift: 平均延时比brpc高1毫秒，受长尾影响不大。 hulu-pbrpc: 走向和UB和thrift类似，但平均延时进一步增加了1毫秒。 gRPC : 初期不错，到长尾区域后表现糟糕，直接有一部分请求超时了。（反复测试都是这样，像是有bug） sofa-pbrpc: 30%的普通请求（上图未显示）被长尾严重干扰。  跨机多client→单server的QPS（越高越好） 本测试运行在多机1上。\n（X轴是client数，Y轴是对应的QPS）\n分析\n brpc: 随着cilent增加，server的QPS在快速增加，有不错的client扩展性。 sofa-pbrpc: 随着client增加，server的QPS也在快速增加，但幅度不如brpc，client扩展性也不错。从16个client到32个client时的提升较小。 hulu-pbrpc: 随着client增加，server的QPS在增加，但幅度进一步小于sofa-pbrpc。 UB: 增加client几乎不能增加server的QPS。 thrift: 平均QPS低于UB，增加client几乎不能增加server的QPS。 gRPC: 垫底、增加client几乎不能增加server的QPS。  跨机多client→单server在固定QPS下的延时CDF（越左越好，越直越好） 本测试运行在多机1上。负载均衡算法为round-robin或RPC默认提供的。由于有32个client且一些RPC的单client能力不佳，我们为每个client仅设定了2500QPS，这是一个真实业务系统能达到的数字。\n本测试中有1%的长尾请求耗时15毫秒，长尾请求的延时不计入结果，因为我们考察的是普通请求是否被及时处理了。\n（X轴是延时（微秒），Y轴是小于X轴延时的请求比例）\n分析\n brpc: 平均延时短，几乎没有被长尾影响。 UB和thrift: 平均延时短，受长尾影响小，平均延时高于brpc sofa-pbrpc: 14%的普通请求被长尾严重干扰。 hulu-pbrpc: 15%的普通请求被长尾严重干扰。 gRPC: 已经完全失控，非常糟糕。  跨机多client→多server在固定QPS下的延时CDF（越左越好，越直越好） 本测试运行在多机2上。20台每台运行4个client，多线程同步访问10台server。负载均衡算法为round-robin或RPC默认提供的。由于gRPC访问多server较麻烦且有很大概率仍表现不佳，这个测试不包含gRPC。\n本测试中有1%的长尾请求耗时10毫秒，长尾请求的延时不计入结果，因为我们考察的是普通请求是否被及时处理了。\n（X轴是延时（微秒），Y轴是小于X轴延时的请求比例）\n分析\n brpc和UB: 平均延时短，几乎没有被长尾影响。 thrift: 平均延时显著高于brpc和UB。 sofa-pbrpc: 2.5%的普通请求被长尾严重干扰。 hulu-pbrpc: 22%的普通请求被长尾严重干扰。  跨机多client→多server→多server在固定QPS下的延时CDF（越左越好，越直越好） 本测试运行在多机2上。14台每台运行4个client，多线程同步访问8台server，这些server还会同步访问另外8台server。负载均衡算法为round-robin或RPC默认提供的。由于gRPC访问多server较麻烦且有很大概率仍表现不佳，这个测试不包含gRPC。\n本测试中有1%的长尾请求耗时10毫秒，长尾请求的延时不计入结果，因为我们考察的是普通请求是否被及时处理了。\n（X轴是延时（微秒），Y轴是小于X轴延时的请求比例）\n分析\n brpc: 平均延时短，几乎没有被长尾影响。 UB: 平均延时短，长尾区域略差于brpc。 thrift: 平均延时显著高于brpc和UB。 sofa-pbrpc: 17%的普通请求被长尾严重干扰，其中2%的请求延时极长。 hulu-pbrpc: 基本消失在视野中，已无法正常工作。  结论 brpc: 在吞吐，平均延时，长尾处理上都表现优秀。\nUB: 平均延时和长尾处理的表现都不错，吞吐的扩展性较差，提高线程数和client数几乎不能提升吞吐。\nthrift: 单机的平均延时和吞吐尚可，多机的平均延时明显高于brpc和UB。吞吐的扩展性较差，提高线程数和client数几乎不能提升吞吐。\nsofa-pbrpc: 处理小包的吞吐尚可，大包的吞吐显著低于其他RPC，延时受长尾影响很大。\nhulu-pbrpc: 单机表现和sofa-pbrpc类似，但多机的延时表现极差。\ngRPC: 几乎在所有参与的测试中垫底，可能它的定位是给google cloud platform的用户提供一个多语言，对网络友好的实现，性能还不是要务。\n","excerpt":"序言 在多核的前提下，性能和线程是紧密联系在一起的。线程间的跳转对高频IO操作的性能有决定性作用: 一次跳转意味着至少3-20微秒的延时，由于每个核心的L1 cache独立（我们的cpu L2 …","ref":"https://brpc.incubator.apache.org/en/docs/benchmark/","title":"Performance benchmark"},{"body":"Prerequisites Before you start to write a MOSN document, create a MOSN document repository and be familiar with the MOSN document structure.\nPage types Documentation The documentation that systematically describe how to use MOSN are maintained by the MOSN team.\nBlogs The periodically published MOSN blogs are contributed by the community.\nNews The news about the MOSN community.\nReleases Release notes about new versions of MOSN.\nTopic structure All topics of the MOSN documentation are saved under the content directory: content/zh for Chinese topics, and content/en for English topics. To create a topic under an existing one, you need to first create a directory and comply with the following naming rules:\n Name all topics without sub-directories as index.md. Name all topics with sub-directories as _index.md.  Topic metadata Each topic has its metadata, which is separated by three hyphens (\u0026quot;-\u0026quot;) between two YAML blocks. The following metadata is required:\ntitle=\u0026#34;Title\u0026#34;linkTitle:\u0026#34;Title\u0026#34;date:2020-02-11weight:1description:\u0026gt;The brief description of the page.Details are described as follows:\n title: The title of the topic. linkTitle: The topic title displayed in the left-side content pane, which is usually consistent with title. date: The date when the topic was created, in the YYYY-MM-dd format. weight: Among topics of the same level, the one with the smallest weight is displayed at the top in the left-side content pane. description: The brief description of the document.  Author information is required for blogs, releases notes, and news articles:\nauthor:\u0026#34;Author information\u0026#34;Note: The value of author can be edited in the Markdown format.\nNaming conventions The URL of a document is determined based on the level of its directory. Document directories are named according to the following conventions:\n Use English names Connect words with hyphens Avoid punctuation marks Minimize the name  ","excerpt":"Prerequisites Before you start to write a MOSN document, create a MOSN document repository and be …","ref":"https://brpc.incubator.apache.org/en/docs/contribute/creating-pages/","title":"Create a page"},{"body":"Format standard  Edit the document body in the Markdown format. Start with Heading 2 in the document body. Use local images, save them to the same directory as that of the index.md file, and use relative paths for references. Specify the languages for all code. Use ordered and unordered lists separately, to avoid format confusion. Enclose a separate URL with \u0026lt;URL\u0026gt;. Enclose uncommon terms and code phrases with grave accents (`).  ","excerpt":"Format standard  Edit the document body in the Markdown format. Start with Heading 2 in the document …","ref":"https://brpc.incubator.apache.org/en/docs/contribute/style-guide/","title":"Format guide"},{"body":"The sidecar pattern is commonly used in service mesh. It is a container design pattern that came into being earlier than service mesh did. This topic will help you have a general idea about the sidecar pattern.\nWhat is the sidecar pattern? In the sidecar pattern, features of applications are separated as processes. As shown in the following figure, the sidecar pattern allows you to add more features alongside every application container, without configuring third-party components or modifying the application code.\nSimilar to a sidecar mounted to a three-wheeled motorcycle, in a software architecture, a sidecar is connected to a primary application and adds extension or advanced features to it. Sidecar applications and primary applications are loosely coupled. This pattern can cover the difference between programming languages, and unify microservice features such as observability, monitoring, logging, configuration, and circuit breaker.\nBenefits Benefits of using the sidecar pattern are as follows:\n  Moves features unrelated to the application business logic into the common infrastructure. This reduces code complexity of microservices.\n  Avoids repeatedly writing configuration files and code for third-party components. This reduces duplicated code in the microservice architecture.\n  Loosens the coupling between the application code and the underlying platform.\n  How does the sidecar pattern work? Sidecar is a container widely used in service mesh. For details, see Service Mesh Architectures. That blog describes the node agent and sidecar service mesh architectures in detail.\nYou can deploy a sidecar service mesh without having to run a new agent on every node, but you will be running multiple copies of an identical sidecar. In sidecar deployments, you have one adjacent container deployed for every application container. This container is a sidecar. The sidecar handles all the network traffic in and out of the application container. In a Kubernetes pod, a sidecar container is deployed alongside the original application container. These two containers share resources such as storage and network. If we take the pod that runs the sidecar container and the application container as a host, these two containers share all host resources.\n","excerpt":"The sidecar pattern is commonly used in service mesh. It is a container design pattern that came …","ref":"https://brpc.incubator.apache.org/en/docs/concept/sidecar-pattern/","title":"Sidecar pattern"},{"body":"","excerpt":"","ref":"https://brpc.incubator.apache.org/en/docs/bvar/","title":"bvar"},{"body":"What is bvar? bvar is a set of counters to record and view miscellaneous statistics conveniently in multi-threaded applications. The implementation reduces cache bouncing by storing data in thread local storage(TLS), being much faster than UbMonitor(a legacy counting library inside Baidu) and even atomic operations in highly contended scenarios. brpc integrates bvar by default, namely all exposed bvars in a server are accessible through /vars, and a single bvar is addressable by /vars/VARNAME. Read vars to know how to query them in brpc servers. brpc extensively use bvar to expose internal status. If you are looking for an utility to collect and display metrics of your application, consider bvar in the first place. bvar definitely can\u0026rsquo;t replace all counters, essentially it moves contentions occurred during write to read: which needs to combine all data written by all threads and becomes much slower than an ordinary read. If read and write on the counter are both frequent or decisions need to be made based on latest values, you should not use bvar.\nTo understand how bvar works, read explaining cacheline first, in which the mentioned counter example is just bvar. When many threads are modifying a counter, each thread writes into its own area without joining the global contention and all private data are combined at read, which is much slower than an ordinary one, but OK for low-frequency logging or display. The much faster and very-little-overhead write enables users to monitor systems from all aspects without worrying about hurting performance. This is the purpose that we designed bvar.\nFollowing graph compares overhead of bvar, atomics, static UbMonitor, dynamic UbMonitor when they\u0026rsquo;re accessed by multiple threads simultaneously. We can see that overhead of bvar is not related to number of threads basically, and being constantly low (~20 nanoseconds). As a contrast, dynamic UbMonitor costs 7 microseconds on each operation when there\u0026rsquo;re 24 threads, which is the overhead of using the bvar for 300 times.\nAdding new bvar Read Quick introduction to know how to add bvar in C++. bvar already provides stats of many process-level and system-level variables by default, which are prefixed with process_ and system_, such as:\nprocess_context_switches_involuntary_second : 14 process_context_switches_voluntary_second : 15760 process_cpu_usage : 0.428 process_cpu_usage_system : 0.142 process_cpu_usage_user : 0.286 process_disk_read_bytes_second : 0 process_disk_write_bytes_second : 260902 process_faults_major : 256 process_faults_minor_second : 14 process_memory_resident : 392744960 system_core_count : 12 system_loadavg_15m : 0.040 system_loadavg_1m : 0.000 system_loadavg_5m : 0.020 and miscellaneous bvars used by brpc itself:\nbthread_switch_second : 20422 bthread_timer_scheduled_second : 4 bthread_timer_triggered_second : 4 bthread_timer_usage : 2.64987e-05 bthread_worker_count : 13 bthread_worker_usage : 1.33733 bvar_collector_dump_second : 0 bvar_collector_dump_thread_usage : 0.000307385 bvar_collector_grab_second : 0 bvar_collector_grab_thread_usage : 1.9699e-05 bvar_collector_pending_samples : 0 bvar_dump_interval : 10 bvar_revision : \u0026quot;34975\u0026quot; bvar_sampler_collector_usage : 0.00106495 iobuf_block_count : 89 iobuf_block_count_hit_tls_threshold : 0 iobuf_block_memory : 729088 iobuf_newbigview_second : 10 New exported files overwrite previous files, which is different from regular logs which append new data.\nMonitoring bvar Turn on dump feature of bvar to export all exposed bvars to files, which are formatted just like above texts: each line is a pair of \u0026ldquo;name\u0026rdquo; and \u0026ldquo;value\u0026rdquo;. Check if there\u0026rsquo;re data under $PWD/monitor/ after enabling dump, for example:\n$ ls monitor/ bvar.echo_client.data bvar.echo_server.data $ tail -5 monitor/bvar.echo_client.data process_swaps : 0 process_time_real : 2580.157680 process_time_system : 0.380942 process_time_user : 0.741887 process_username : \u0026quot;gejun\u0026quot; The monitoring system should combine data on every single machine periodically and merge them together to provide on-demand queries. Take the \u0026ldquo;noah\u0026rdquo; system inside Baidu as an example, variables defined by bvar appear as metrics in noah, which can be checked by users to view historical curves.\nExport to Prometheus To export to Prometheus, set the path in scraping target url to /brpc_metrics. For example, if brpc server is running on localhost:8080, the scraping target should be 127.0.0.1:8080/brpc_metrics.\n","excerpt":"What is bvar? bvar is a set of counters to record and view miscellaneous statistics conveniently in …","ref":"https://brpc.incubator.apache.org/en/docs/bvar/bvar/","title":"bvar"},{"body":"Quick introduction #include \u0026lt;bvar/bvar.h\u0026gt; namespace foo { namespace bar { // bvar::Adder\u0026lt;T\u0026gt;用于累加，下面定义了一个统计read error总数的Adder。 bvar::Adder\u0026lt;int\u0026gt; g_read_error; // 把bvar::Window套在其他bvar上就可以获得时间窗口内的值。 bvar::Window\u0026lt;bvar::Adder\u0026lt;int\u0026gt; \u0026gt; g_read_error_minute(\u0026#34;foo_bar\u0026#34;, \u0026#34;read_error\u0026#34;, \u0026amp;g_read_error, 60); // ^ ^ ^ // 前缀 监控项名称 60秒,忽略则为10秒  // bvar::LatencyRecorder是一个复合变量，可以统计：总量、qps、平均延时，延时分位值，最大延时。 bvar::LatencyRecorder g_write_latency(\u0026#34;foo_bar\u0026#34;, \u0026#34;write\u0026#34;); // ^ ^ // 前缀 监控项，别加latency！LatencyRecorder包含多个bvar，它们会加上各自的后缀，比如write_qps, write_latency等等。  // 定义一个统计“已推入task”个数的变量。 bvar::Adder\u0026lt;int\u0026gt; g_task_pushed(\u0026#34;foo_bar\u0026#34;, \u0026#34;task_pushed\u0026#34;); // 把bvar::PerSecond套在其他bvar上可以获得时间窗口内*平均每秒*的值，这里是每秒内推入task的个数。 bvar::PerSecond\u0026lt;bvar::Adder\u0026lt;int\u0026gt; \u0026gt; g_task_pushed_second(\u0026#34;foo_bar\u0026#34;, \u0026#34;task_pushed_second\u0026#34;, \u0026amp;g_task_pushed); // ^ ^ // 和Window不同，PerSecond会除以时间窗口的大小. 时间窗口是最后一个参数，这里没填，就是默认10秒。  } // bar } // foo 在应用的地方：\n// 碰到read error foo::bar::g_read_error \u0026lt;\u0026lt; 1; // write_latency是23ms foo::bar::g_write_latency \u0026lt;\u0026lt; 23; // 推入了1个task foo::bar::g_task_pushed \u0026lt;\u0026lt; 1; 注意Window\u0026lt;\u0026gt;和PerSecond\u0026lt;\u0026gt;都是衍生变量，会自动更新，你不用给它们推值。你当然也可以把bvar作为成员变量或局部变量。\n常用的bvar有：\n bvar::Adder\u0026lt;T\u0026gt; : 计数器，默认0，varname \u0026laquo; N相当于varname += N。 bvar::Maxer\u0026lt;T\u0026gt; : 求最大值，默认std::numeric_limits::min()，varname \u0026laquo; N相当于varname = max(varname, N)。 bvar::Miner\u0026lt;T\u0026gt; : 求最小值，默认std::numeric_limits::max()，varname \u0026laquo; N相当于varname = min(varname, N)。 bvar::IntRecorder : 求自使用以来的平均值。注意这里的定语不是“一段时间内”。一般要通过Window衍生出时间窗口内的平均值。 bvar::Window\u0026lt;VAR\u0026gt; : 获得某个bvar在一段时间内的累加值。Window衍生于已存在的bvar，会自动更新。 bvar::PerSecond\u0026lt;VAR\u0026gt; : 获得某个bvar在一段时间内平均每秒的累加值。PerSecond也是会自动更新的衍生变量。 bvar::LatencyRecorder : 专用于记录延时和qps的变量。输入延时，平均延时/最大延时/qps/总次数 都有了。  **确认变量名是全局唯一的！**否则会曝光失败，如果-bvar_abort_on_same_name为true，程序会直接abort。\n程序中有来自各种模块不同的bvar，为避免重名，建议如此命名：模块_类名_指标\n 模块一般是程序名，可以加上产品线的缩写，比如inf_ds，ecom_retrbs等等。 类名一般是类名或函数名，比如storage_manager, file_transfer, rank_stage1等等。 指标一般是count，qps，latency这类。  一些正确的命名如下：\niobuf_block_count : 29 # 模块=iobuf 类名=block 指标=count iobuf_block_memory : 237568 # 模块=iobuf 类名=block 指标=memory process_memory_resident : 34709504 # 模块=process 类名=memory 指标=resident process_memory_shared : 6844416 # 模块=process 类名=memory 指标=shared rpc_channel_connection_count : 0 # 模块=rpc 类名=channel_connection 指标=count rpc_controller_count : 1 # 模块=rpc 类名=controller 指标=count rpc_socket_count : 6 # 模块=rpc 类名=socket 指标=count 目前bvar会做名字归一化，不管你打入的是foo::BarNum, foo.bar.num, foo bar num , foo-bar-num，最后都是foo_bar_num。\n关于指标：\n 个数以_count为后缀，比如request_count, error_count。 每秒的个数以_second为后缀，比如request_second, process_inblocks_second，已经足够明确，不用写成_count_second或_per_second。 每分钟的个数以_minute为后缀，比如request_minute, process_inblocks_minute  如果需要使用定义在另一个文件中的计数器，需要在头文件中声明对应的变量。\nnamespace foo { namespace bar { // 注意g_read_error_minute和g_task_pushed_per_second都是衍生的bvar，会自动更新，不要声明。 extern bvar::Adder\u0026lt;int\u0026gt; g_read_error; extern bvar::LatencyRecorder g_write_latency; extern bvar::Adder\u0026lt;int\u0026gt; g_task_pushed; } // bar } // foo 不要跨文件定义全局Window或PerSecond。不同编译单元中全局变量的初始化顺序是未定义的。在foo.cpp中定义Adder\u0026lt;int\u0026gt; foo_count，在foo_qps.cpp中定义PerSecond\u0026lt;Adder\u0026lt;int\u0026gt; \u0026gt; foo_qps(\u0026amp;foo_count);是错误的做法。\nAbout thread-safety:\n bvar是线程兼容的。你可以在不同的线程里操作不同的bvar。比如你可以在多个线程中同时expose或hide不同的bvar，它们会合理地操作需要共享的全局数据，是安全的。 除了读写接口，bvar的其他函数都是线程不安全的：比如说你不能在多个线程中同时expose或hide同一个bvar，这很可能会导致程序crash。一般来说，读写之外的其他接口也没有必要在多个线程中同时操作。  计时可以使用butil::Timer，接口如下：\n#include \u0026lt;butil/time.h\u0026gt;namespace butil { class Timer { public: enum TimerType { STARTED }; Timer(); // butil::Timer tm(butil::Timer::STARTED); // tm is already started after creation.  explicit Timer(TimerType); // Start this timer  void start(); // Stop this timer  void stop(); // Get the elapse from start() to stop().  int64_t n_elapsed() const; // in nanoseconds  int64_t u_elapsed() const; // in microseconds  int64_t m_elapsed() const; // in milliseconds  int64_t s_elapsed() const; // in seconds }; } // namespace butil bvar::Variable Variable是所有bvar的基类，主要提供全局注册，列举，查询等功能。\n用户以默认参数建立一个bvar时，这个bvar并未注册到任何全局结构中，在这种情况下，bvar纯粹是一个更快的计数器。我们称把一个bvar注册到全局表中的行为为“曝光”，可通过expose函数曝光：\n// Expose this variable globally so that it\u0026#39;s counted in following functions: // list_exposed // count_exposed // describe_exposed // find_exposed // Return 0 on success, -1 otherwise. int expose(const butil::StringPiece\u0026amp; name); int expose_as(const butil::StringPiece\u0026amp; prefix, const butil::StringPiece\u0026amp; name); 全局曝光后的bvar名字便为name或prefix + name，可通过以_exposed为后缀的static函数查询。比如Variable::describe_exposed(name)会返回名为name的bvar的描述。\n当相同名字的bvar已存在时，expose会打印FATAL日志并返回-1。如果选项**\u0026ndash;bvar_abort_on_same_name**设为true (默认是false)，程序会直接abort。\n下面是一些曝光bvar的例子：\nbvar::Adder\u0026lt;int\u0026gt; count1; count1 \u0026lt;\u0026lt; 10 \u0026lt;\u0026lt; 20 \u0026lt;\u0026lt; 30; // values add up to 60. count1.expose(\u0026#34;count1\u0026#34;); // expose the variable globally CHECK_EQ(\u0026#34;60\u0026#34;, bvar::Variable::describe_exposed(\u0026#34;count1\u0026#34;)); count1.expose(\u0026#34;another_name_for_count1\u0026#34;); // expose the variable with another name CHECK_EQ(\u0026#34;\u0026#34;, bvar::Variable::describe_exposed(\u0026#34;count1\u0026#34;)); CHECK_EQ(\u0026#34;60\u0026#34;, bvar::Variable::describe_exposed(\u0026#34;another_name_for_count1\u0026#34;)); bvar::Adder\u0026lt;int\u0026gt; count2(\u0026#34;count2\u0026#34;); // exposed in constructor directly CHECK_EQ(\u0026#34;0\u0026#34;, bvar::Variable::describe_exposed(\u0026#34;count2\u0026#34;)); // default value of Adder\u0026lt;int\u0026gt; is 0  bvar::Status\u0026lt;std::string\u0026gt; status1(\u0026#34;count2\u0026#34;, \u0026#34;hello\u0026#34;); // the name conflicts. if -bvar_abort_on_same_name is true,  // program aborts, otherwise a fatal log is printed. 为避免重名，bvar的名字应加上前缀，建议为\u0026lt;namespace\u0026gt;_\u0026lt;module\u0026gt;_\u0026lt;name\u0026gt;。为了方便使用，我们提供了expose_as函数，接收一个前缀。\n// Expose this variable with a prefix. // Example: // namespace foo { // namespace bar { // class ApplePie { // ApplePie() { // // foo_bar_apple_pie_error // _error.expose_as(\u0026#34;foo_bar_apple_pie\u0026#34;, \u0026#34;error\u0026#34;); // } // private: // bvar::Adder\u0026lt;int\u0026gt; _error; // }; // } // foo // } // bar int expose_as(const butil::StringPiece\u0026amp; prefix, const butil::StringPiece\u0026amp; name); Export all variables 最常见的导出需求是通过HTTP接口查询和写入本地文件。前者在brpc中通过/vars服务提供，后者则已实现在bvar中，默认不打开。有几种方法打开这个功能：\n 用gflags解析输入参数，在程序启动时加入-bvar_dump，或在brpc中也可通过/flags服务在启动后动态修改。gflags的解析方法如下，在main函数处添加如下代码:  #include \u0026lt;gflags/gflags.h\u0026gt; ... int main(int argc, char* argv[]) { google::ParseCommandLineFlags(\u0026amp;argc, \u0026amp;argv, true/*表示把识别的参数从argc/argv中删除*/); ... }  不想用gflags解析参数，希望直接在程序中默认打开，在main函数处添加如下代码：  #include \u0026lt;gflags/gflags.h\u0026gt;... int main(int argc, char* argv[]) { if (google::SetCommandLineOption(\u0026#34;bvar_dump\u0026#34;, \u0026#34;true\u0026#34;).empty()) { LOG(FATAL) \u0026lt;\u0026lt; \u0026#34;Fail to enable bvar dump\u0026#34;; } ... } dump功能由如下gflags控制：\n   名称 默认值 作用     bvar_dump false Create a background thread dumping all bvar periodically, all bvar_dump_* flags are not effective when this flag is off   bvar_dump_exclude \u0026quot;\u0026quot; Dump bvar excluded from these wildcards(separated by comma), empty means no exclusion   bvar_dump_file monitor/bvar..data Dump bvar into this file   bvar_dump_include \u0026quot;\u0026quot; Dump bvar matching these wildcards(separated by comma), empty means including all   bvar_dump_interval 10 Seconds between consecutive dump   bvar_dump_prefix \u0026lt;app\u0026gt; Every dumped name starts with this prefix   bvar_dump_tabs \u0026lt;check the code\u0026gt; Dump bvar into different tabs according to the filters (seperated by semicolon), format: *(tab_name=wildcards)    当bvar_dump_file不为空时，程序会启动一个后台导出线程以bvar_dump_interval指定的间隔更新bvar_dump_file，其中包含了被bvar_dump_include匹配且不被bvar_dump_exclude匹配的所有bvar。\n比如我们把所有的gflags修改为下图：\n导出文件为：\n$ cat bvar.echo_server.data rpc_server_8002_builtin_service_count : 20 rpc_server_8002_connection_count : 1 rpc_server_8002_nshead_service_adaptor : brpc::policy::NovaServiceAdaptor rpc_server_8002_service_count : 1 rpc_server_8002_start_time : 2015/07/24-21:08:03 rpc_server_8002_uptime_ms : 14740954 像”iobuf_block_count : 8”被bvar_dump_include过滤了，“rpc_server_8002_error : 0”则被bvar_dump_exclude排除了。\n如果你的程序没有使用brpc，仍需要动态修改gflag（一般不需要），可以调用google::SetCommandLineOption()，如下所示：\n#include \u0026lt;gflags/gflags.h\u0026gt;... if (google::SetCommandLineOption(\u0026#34;bvar_dump_include\u0026#34;, \u0026#34;*service*\u0026#34;).empty()) { LOG(ERROR) \u0026lt;\u0026lt; \u0026#34;Fail to set bvar_dump_include\u0026#34;; return -1; } LOG(INFO) \u0026lt;\u0026lt; \u0026#34;Successfully set bvar_dump_include to *service*\u0026#34;; 请勿直接设置FLAGS_bvar_dump_file / FLAGS_bvar_dump_include / FLAGS_bvar_dump_exclude。 一方面这些gflag类型都是std::string，直接覆盖是线程不安全的；另一方面不会触发validator（检查正确性的回调），所以也不会启动后台导出线程。\n用户也可以使用dump_exposed函数自定义如何导出进程中的所有已曝光的bvar：\n// Implement this class to write variables into different places. // If dump() returns false, Variable::dump_exposed() stops and returns -1. class Dumper { public: virtual bool dump(const std::string\u0026amp; name, const butil::StringPiece\u0026amp; description) = 0; }; // Options for Variable::dump_exposed(). struct DumpOptions { // Contructed with default options.  DumpOptions(); // If this is true, string-type values will be quoted.  bool quote_string; // The ? in wildcards. Wildcards in URL need to use another character  // because ? is reserved.  char question_mark; // Separator for white_wildcards and black_wildcards.  char wildcard_separator; // Name matched by these wildcards (or exact names) are kept.  std::string white_wildcards; // Name matched by these wildcards (or exact names) are skipped.  std::string black_wildcards; }; class Variable { ... ... // Find all exposed variables matching `white_wildcards\u0026#39; but  // `black_wildcards\u0026#39; and send them to `dumper\u0026#39;.  // Use default options when `options\u0026#39; is NULL.  // Return number of dumped variables, -1 on error.  static int dump_exposed(Dumper* dumper, const DumpOptions* options); }; bvar::Reducer Reducer用二元运算符把多个值合并为一个值，运算符需满足结合律，交换律，没有副作用。只有满足这三点，我们才能确保合并的结果不受线程私有数据如何分布的影响。像减法就不满足结合律和交换律，它无法作为此处的运算符。\n// Reduce multiple values into one with `Op\u0026#39;: e1 Op e2 Op e3 ... // `Op\u0026#39; shall satisfy: // - associative: a Op (b Op c) == (a Op b) Op c // - commutative: a Op b == b Op a; // - no side effects: a Op b never changes if a and b are fixed. // otherwise the result is undefined. template \u0026lt;typename T, typename Op\u0026gt; class Reducer : public Variable; reducer \u0026laquo; e1 \u0026laquo; e2 \u0026laquo; e3的作用等价于reducer = e1 op e2 op e3。\n常见的Redcuer子类有bvar::Adder, bvar::Maxer, bvar::Miner。\nbvar::Adder 顾名思义，用于累加，Op为+。\nbvar::Adder\u0026lt;int\u0026gt; value; value \u0026lt;\u0026lt; 1 \u0026lt;\u0026lt; 2 \u0026lt;\u0026lt; 3 \u0026lt;\u0026lt; -4; CHECK_EQ(2, value.get_value()); bvar::Adder\u0026lt;double\u0026gt; fp_value; // 可能有warning fp_value \u0026lt;\u0026lt; 1.0 \u0026lt;\u0026lt; 2.0 \u0026lt;\u0026lt; 3.0 \u0026lt;\u0026lt; -4.0; CHECK_DOUBLE_EQ(2.0, fp_value.get_value()); Adder\u0026lt;\u0026gt;可用于非基本类型，对应的类型至少要重载T operator+(T, T)。一个已经存在的例子是std::string，下面的代码会把string拼接起来：\n// This is just proof-of-concept, don\u0026#39;t use it for production code because it makes a // bunch of temporary strings which is not efficient, use std::ostringstream instead. bvar::Adder\u0026lt;std::string\u0026gt; concater; std::string str1 = \u0026#34;world\u0026#34;; concater \u0026lt;\u0026lt; \u0026#34;hello \u0026#34; \u0026lt;\u0026lt; str1; CHECK_EQ(\u0026#34;hello world\u0026#34;, concater.get_value()); bvar::Maxer 用于取最大值，运算符为std::max。\nbvar::Maxer\u0026lt;int\u0026gt; value; value \u0026lt;\u0026lt; 1 \u0026lt;\u0026lt; 2 \u0026lt;\u0026lt; 3 \u0026lt;\u0026lt; -4; CHECK_EQ(3, value.get_value()); Since Maxer\u0026lt;\u0026gt; use std::numeric_limits::min() as the identity, it cannot be applied to generic types unless you specialized std::numeric_limits\u0026lt;\u0026gt; (and overloaded operator\u0026lt;, yes, not operator\u0026gt;).\nbvar::Miner 用于取最小值，运算符为std::min。\nbvar::Maxer\u0026lt;int\u0026gt; value; value \u0026lt;\u0026lt; 1 \u0026lt;\u0026lt; 2 \u0026lt;\u0026lt; 3 \u0026lt;\u0026lt; -4; CHECK_EQ(-4, value.get_value()); Since Miner\u0026lt;\u0026gt; use std::numeric_limits::max() as the identity, it cannot be applied to generic types unless you specialized std::numeric_limits\u0026lt;\u0026gt; (and overloaded operator\u0026lt;).\nbvar::IntRecorder 用于计算平均值。\n// For calculating average of numbers. // Example: // IntRecorder latency; // latency \u0026lt;\u0026lt; 1 \u0026lt;\u0026lt; 3 \u0026lt;\u0026lt; 5; // CHECK_EQ(3, latency.average()); class IntRecorder : public Variable; bvar::LatencyRecorder 专用于计算latency和qps的计数器。只需填入latency数据，就能获得latency / max_latency / qps / count。统计窗口是最后一个参数，不填为bvar_dump_interval（这里没填）。\n注意：LatencyRecorder没有继承Variable，而是多个bvar的组合。\nLatencyRecorder write_latency(\u0026#34;table2_my_table_write\u0026#34;); // produces 4 variables:  // table2_my_table_write_latency  // table2_my_table_write_max_latency  // table2_my_table_write_qps  // table2_my_table_write_count // In your write function write_latency \u0026lt;\u0026lt; the_latency_of_write; bvar::Window 获得之前一段时间内的统计值。Window不能独立存在，必须依赖于一个已有的计数器。Window会自动更新，不用给它发送数据。出于性能考虑，Window的数据来自于每秒一次对原计数器的采样，在最差情况下，Window的返回值有1秒的延时。\n// Get data within a time window. // The time unit is 1 second fixed. // Window relies on other bvar which should be constructed before this window and destructs after this window. // R must: // - have get_sampler() (not require thread-safe) // - defined value_type and sampler_type template \u0026lt;typename R\u0026gt; class Window : public Variable; bvar::PerSecond 获得之前一段时间内平均每秒的统计值。它和Window基本相同，除了返回值会除以时间窗口之外。\nbvar::Adder\u0026lt;int\u0026gt; sum; // sum_per_second.get_value()是sum在之前60秒内*平均每秒*的累加值，省略最后一个时间窗口的话默认为bvar_dump_interval。 bvar::PerSecond\u0026lt;bvar::Adder\u0026lt;int\u0026gt; \u0026gt; sum_per_second(\u0026amp;sum, 60); PerSecond并不总是有意义\n上面的代码中没有Maxer，因为一段时间内的最大值除以时间窗口是没有意义的。\nbvar::Maxer\u0026lt;int\u0026gt; max_value; // 错误！最大值除以时间是没有意义的 bvar::PerSecond\u0026lt;bvar::Maxer\u0026lt;int\u0026gt; \u0026gt; max_value_per_second_wrong(\u0026amp;max_value); // 正确，把Window的时间窗口设为1秒才是正确的做法 bvar::Window\u0026lt;bvar::Maxer\u0026lt;int\u0026gt; \u0026gt; max_value_per_second(\u0026amp;max_value, 1); 和Window的差别 比如要统计内存在上一分钟内的变化，用Window\u0026lt;\u0026gt;的话，返回值的含义是”上一分钟内存增加了18M”，用PerSecond\u0026lt;\u0026gt;的话，返回值的含义是“上一分钟平均每秒增加了0.3M”。\nWindow的优点是精确值，适合一些比较小的量，比如“上一分钟的错误数“，如果这用PerSecond的话，得到可能是”上一分钟平均每秒产生了0.0167个错误\u0026quot;，这相比于”上一分钟有1个错误“显然不够清晰。另外一些和时间无关的量也要用Window，比如统计上一分钟cpu占用率的方法是用一个Adder同时累加cpu时间和真实时间，然后用Window获得上一分钟的cpu时间和真实时间，两者相除就得到了上一分钟的cpu占用率，这和时间无关，用PerSecond会产生错误的结果。\nbvar::Status 记录和显示一个值，拥有额外的set_value函数。\n// Display a rarely or periodically updated value. // Usage: // bvar::Status\u0026lt;int\u0026gt; foo_count1(17); // foo_count1.expose(\u0026#34;my_value\u0026#34;); // // bvar::Status\u0026lt;int\u0026gt; foo_count2; // foo_count2.set_value(17); // // bvar::Status\u0026lt;int\u0026gt; foo_count3(\u0026#34;my_value\u0026#34;, 17); // // Notice that Tp needs to be std::string or acceptable by boost::atomic\u0026lt;Tp\u0026gt;. template \u0026lt;typename Tp\u0026gt; class Status : public Variable; bvar::PassiveStatus 按需显示值。在一些场合中，我们无法set_value或不知道以何种频率set_value，更适合的方式也许是当需要显示时才打印。用户传入打印回调函数实现这个目的。\n// Display a updated-by-need value. This is done by passing in an user callback // which is called to produce the value. // Example: // int print_number(void* arg) { // ... // return 5; // } // // // number1 : 5 // bvar::PassiveStatus status1(\u0026#34;number1\u0026#34;, print_number, arg); // // // foo_number2 : 5 // bvar::PassiveStatus status2(typeid(Foo), \u0026#34;number2\u0026#34;, print_number, arg); template \u0026lt;typename Tp\u0026gt; class PassiveStatus : public Variable; 虽然很简单，但PassiveStatus是最有用的bvar之一，因为很多统计量已经存在，我们不需要再次存储它们，而只要按需获取。比如下面的代码声明了一个在linux下显示进程用户名的bvar：\nstatic void get_username(std::ostream\u0026amp; os, void*) { char buf[32]; if (getlogin_r(buf, sizeof(buf)) == 0) { buf[sizeof(buf)-1] = \u0026#39;\\0\u0026#39;; os \u0026lt;\u0026lt; buf; } else { os \u0026lt;\u0026lt; \u0026#34;unknown\u0026#34;; } } PassiveStatus\u0026lt;std::string\u0026gt; g_username(\u0026#34;process_username\u0026#34;, get_username, NULL); bvar::GFlag Expose important gflags as bvar so that they\u0026rsquo;re monitored (in noah).\nDEFINE_int32(my_flag_that_matters, 8, \u0026#34;...\u0026#34;); // Expose the gflag as *same-named* bvar so that it\u0026#39;s monitored (in noah). static bvar::GFlag s_gflag_my_flag_that_matters(\u0026#34;my_flag_that_matters\u0026#34;); // ^ // the gflag name  // Expose the gflag as a bvar named \u0026#34;foo_bar_my_flag_that_matters\u0026#34;. static bvar::GFlag s_gflag_my_flag_that_matters_with_prefix(\u0026#34;foo_bar\u0026#34;, \u0026#34;my_flag_that_matters\u0026#34;); ","excerpt":"Quick introduction #include \u0026lt;bvar/bvar.h\u0026gt; namespace foo { namespace bar { // …","ref":"https://brpc.incubator.apache.org/en/docs/bvar/bvar-c++/","title":"bvar c++"},{"body":"This topic describes the traffic hijacking solution when MOSN is used as a sidecar.\nWhen MOSN is deployed as a sidecar in the same pod as that of the business application container, both inbound and outbound service requests of the business application must be handled by the sidecar. UUnlike the Istio community that uses iptables for transparent traffic hijacking, MOSN currently uses a traffic takeover solution and is actively exploring a transparent hijacking solution for large amounts of traffic.\nTraffic takeover MOSN uses the following traffic takeover solution, instead of the iptables-based traffic hijacking solution used by the Istio community:\n Assume that the server runs on a machine with an IP address of 1.2.3.4 and listens on port 20880. The server will first send a service registration request to its sidecar, to notify it of the service to be registered, as well as the IP address and port number (1.2.3.4:20880). The server\u0026rsquo;s sidecar will send a service registration request to the service registry (for example, SOFA Registry), to notify it of the service to be registered as well as the IP address and port number. Note that the port to be registered is not the port (20880) of the business application. Instead, it should be the port (for example, 20881) on which the sidecar listens. The caller sends a service subscription request to its own sidecar, to notify it of the service to be subscribed to. The caller\u0026rsquo;s sidecar then pushes the service endpoint to the caller. Note that the service endpoint here is the local IP address of the caller and the port (for example, 20882) on which the caller\u0026rsquo;s sidecar listens. The caller\u0026rsquo;s sidecar sends a service subscription request to the service registry (for example, SOFA Registry), to notify it of the service to be subscribed to. The service registry (for example, SOFA Registry) pushes the service endpoint (1.2.3.4:20881) to the caller\u0026rsquo;s sidecar.  Service call procedure After going through the above service discovery procedure, the traffic forwarding procedure is quite easy to understand:\n The caller receives the so called \u0026ldquo;service endpoint\u0026rdquo; 127.0.0.1:20882, and then sends a service call to this endpoint. After receiving the request from the caller, the caller\u0026rsquo;s sidecar parses the request header to identify the service to be called. Then the sidecar retrieves the endpoint previously returned by the service registry, and then sends a real call to this endpoint: 1.2.3.4:20881. After receiving the request from the caller\u0026rsquo;s sidecar, the server\u0026rsquo;s sidecar processes the request and then sends the request to the server:127.0.0.1:20880.  Transparent hijacking In the above service registration procedure, the server endpoint is replaced with the local sidecar endpoint to implement lightweight \u0026ldquo;traffic hijacking\u0026rdquo;. This mode works well in scenarios where a service registry is available and both the caller and the server use the same SDK. Traffic hijacking is not possible unless both conditions are met. To solve this problem, we introduced transparent hijacking.\niptables-based traffic hijacking iptables redirect traffic by using the REDIRECT action of the NAT table. A new connection will be triggered at the netfilter layer through the SYN packet. When subsequent packets arrive at the netfilter layer, netfilter looks for the corresponding connections, and modifies the destination addresses and ports. The original destination address is recorded when the new connection is being established, and the application can obtain the real destination addresses of the packets through SOL_IP and SO_ORIGINAL_DST.\nThe following figure shows the principle of iptables-based traffic hijacking.\nDrawbacks of iptables-based traffic hijacking Currently, iptables-based traffic hijacking used by Istio has the following drawbacks:\n Connections need to be tracked by the conntrack module. When there are a large number of connections, the resource consumption will be high, and the track table may become full. To address this problem, some industry practitioners, such as Alibaba, have disabled the conntrack module. iptables is a common module that takes effect globally, and related modifications cannot be explicitly prohibited, resulting in poor controllability. iptables-based traffic redirection is essentially data exchange through loopback. The outbound traffic progresses through the stack twice, degrading the forwarding performance in high-concurrency scenarios.  The above drawbacks do not exist in every scenario. For example, iptables is a simple and desirable solution for scenarios where the number of connections is small and no NAT table is used. To adapt to more scenarios, transparent hijacking needs to be optimized to eliminate the above drawbacks.\nOptimization for transparent hijacking Use TProxy to handle inbound traffic\nTProxy can be used to redirect inbound traffic without modifying the destination IP address/port number in messages. No connection needs to be tracked, and the conntrack module does not need to create large numbers of connections. TProxy is not suitable for handling outbound traffic due to the limit of the kernel version. Currently, Istio supports using TProxy to handle inbound traffic.\nUse hook connect to handle outbound traffic\nTo adapt to more application scenarios, hook connect is used to handle outbound traffic. The implementation principle is as follows:\nRegardless of which transparent hijacking solution is adopted, the real destination IP address/port number needs to be identified. In the iptables solution, the destination address is identified by calling the getsockopt function. TProxy can directly read the address. In the hook connect solution, the sidecar can also read the original IP/port number by calling the getsockopt function.\nAfter implementing transparent hijacking, if the kernel version meets the requirement (4.16 or later), we can use sockmap to shorten the message processing path, and improve the forwarding performance in the outbound direction.\nSummary An application can hijack traffic in combination with the registry when publishing/subscribing to services through the registry. In transparent hijacking scenarios, we can use iptables to redirect traffic under low performance pressure. TProxy and hook connect can be used under high concurrency pressure.\n","excerpt":"This topic describes the traffic hijacking solution when MOSN is used as a sidecar.\nWhen MOSN is …","ref":"https://brpc.incubator.apache.org/en/docs/concept/traffic-hijack/","title":"Traffic hijacking"},{"body":"","excerpt":"","ref":"https://brpc.incubator.apache.org/en/docs/concept/","title":"Architecture and principle"},{"body":"","excerpt":"","ref":"https://brpc.incubator.apache.org/en/docs/bthread/","title":"bthread"},{"body":"bthread是brpc使用的M:N线程库，目的是在提高程序的并发度的同时，降低编码难度，并在核数日益增多的CPU上提供更好的scalability和cache locality。”M:N“是指M个bthread会映射至N个pthread，一般M远大于N。由于linux当下的pthread实现(NPTL)是1:1的，M个bthread也相当于映射至N个LWP。bthread的前身是Distributed Process(DP)中的fiber，一个N:1的合作式线程库，等价于event-loop库，但写的是同步代码。\nGoals  用户可以延续同步的编程模式，能在数百纳秒内建立bthread，可以用多种原语同步。 bthread所有接口可在pthread中被调用并有合理的行为，使用bthread的代码可以在pthread中正常执行。 能充分利用多核。 better cache locality, supporting NUMA is a plus.  NonGoals  提供pthread的兼容接口，只需链接即可使用。拒绝理由: bthread没有优先级，不适用于所有的场景，链接的方式容易使用户在不知情的情况下误用bthread，造成bug。 覆盖各类可能阻塞的glibc函数和系统调用，让原本阻塞系统线程的函数改为阻塞bthread。拒绝理由:  bthread阻塞可能切换系统线程，依赖系统TLS的函数的行为未定义。 和阻塞pthread的函数混用时可能死锁。 这类hook函数本身的效率一般更差，因为往往还需要额外的系统调用，如epoll。但这类覆盖对N:1合作式线程库(fiber)有一定意义：虽然函数本身慢了，但若不覆盖会更慢（系统线程阻塞会导致所有fiber阻塞）。   修改内核让pthread支持同核快速切换。拒绝理由: 拥有大量pthread后，每个线程对资源的需求被稀释了，基于thread-local cache的代码效果会很差，比如tcmalloc。而独立的bthread不会有这个问题，因为它最终还是被映射到了少量的pthread。bthread相比pthread的性能提升很大一部分来自更集中的线程资源。另一个考量是可移植性，bthread更倾向于纯用户态代码。  FAQ Q：bthread是协程(coroutine)吗？ 不是。我们常说的协程特指N:1线程库，即所有的协程运行于一个系统线程中，计算能力和各类eventloop库等价。由于不跨线程，协程之间的切换不需要系统调用，可以非常快(100ns-200ns)，受cache一致性的影响也小。但代价是协程无法高效地利用多核，代码必须非阻塞，否则所有的协程都被卡住，对开发者要求苛刻。协程的这个特点使其适合写运行时间确定的IO服务器，典型如http server，在一些精心调试的场景中，可以达到非常高的吞吐。但百度内大部分在线服务的运行时间并不确定，且很多检索由几十人合作完成，一个缓慢的函数会卡住所有的协程。在这点上eventloop是类似的，一个回调卡住整个loop就卡住了，比如ubaserver（注意那个a，不是ubserver）是百度对异步框架的尝试，由多个并行的eventloop组成，真实表现糟糕：回调里打日志慢一些，访问redis卡顿，计算重一点，等待中的其他请求就会大量超时。所以这个框架从未流行起来。\nbthread是一个M:N线程库，一个bthread被卡住不会影响其他bthread。关键技术两点：work stealing调度和butex，前者让bthread更快地被调度到更多的核心上，后者让bthread和pthread可以相互等待和唤醒。这两点协程都不需要。更多线程的知识查看这里。\nQ: 我应该在程序中多使用bthread吗？ 不应该。除非你需要在一次RPC过程中让一些代码并发运行，你不应该直接调用bthread函数，把这些留给brpc做更好。\nQ：bthread和pthread worker如何对应？ pthread worker在任何时间只会运行一个bthread，当前bthread挂起时，pthread worker先尝试从本地runqueue弹出一个待运行的bthread，若没有，则随机偷另一个worker的待运行bthread，仍然没有才睡眠并会在有新的待运行bthread时被唤醒。\nQ：bthread中能调用阻塞的pthread或系统函数吗？ 可以，只阻塞当前pthread worker。其他pthread worker不受影响。\nQ：一个bthread阻塞会影响其他bthread吗？ 不影响。若bthread因bthread API而阻塞，它会把当前pthread worker让给其他bthread。若bthread因pthread API或系统函数而阻塞，当前pthread worker上待运行的bthread会被其他空闲的pthread worker偷过去运行。\nQ：pthread中可以调用bthread API吗？ 可以。bthread API在bthread中被调用时影响的是当前bthread，在pthread中被调用时影响的是当前pthread。使用bthread API的代码可以直接运行在pthread中。\nQ：若有大量的bthread调用了阻塞的pthread或系统函数，会影响RPC运行么？ 会。比如有8个pthread worker，当有8个bthread都调用了系统usleep()后，处理网络收发的RPC代码就暂时无法运行了。只要阻塞时间不太长, 这一般没什么影响, 毕竟worker都用完了, 除了排队也没有什么好方法. 在brpc中用户可以选择调大worker数来缓解问题, 在server端可设置ServerOptions.num_threads或-bthread_concurrency, 在client端可设置-bthread_concurrency.\n那有没有完全规避的方法呢?\n 一个容易想到的方法是动态增加worker数. 但实际未必如意, 当大量的worker同时被阻塞时, 它们很可能在等待同一个资源(比如同一把锁), 增加worker可能只是增加了更多的等待者. 那区分io线程和worker线程? io线程专门处理收发, worker线程调用用户逻辑, 即使worker线程全部阻塞也不会影响io线程. 但增加一层处理环节(io线程)并不能缓解拥塞, 如果worker线程全部卡住, 程序仍然会卡住, 只是卡的地方从socket缓冲转移到了io线程和worker线程之间的消息队列. 换句话说, 在worker卡住时, 还在运行的io线程做的可能是无用功. 事实上, 这正是上面提到的没什么影响真正的含义. 另一个问题是每个请求都要从io线程跳转至worker线程, 增加了一次上下文切换, 在机器繁忙时, 切换都有一定概率无法被及时调度, 会导致更多的延时长尾. 一个实际的解决方法是限制最大并发, 只要同时被处理的请求数低于worker数, 自然可以规避掉\u0026quot;所有worker被阻塞\u0026quot;的情况. 另一个解决方法当被阻塞的worker超过阈值时(比如8个中的6个), 就不在原地调用用户代码了, 而是扔到一个独立的线程池中运行. 这样即使用户代码全部阻塞, 也总能保留几个worker处理rpc的收发. 不过目前bthread模式并没有这个机制, 但类似的机制在打开pthread模式时已经被实现了. 那像上面提到的, 这个机制是不是在用户代码都阻塞时也在做\u0026quot;无用功\u0026quot;呢? 可能是的. 但这个机制更多是为了规避在一些极端情况下的死锁, 比如所有的用户代码都lock在一个pthread mutex上, 并且这个mutex需要在某个RPC回调中unlock, 如果所有的worker都被阻塞, 那么就没有线程来处理RPC回调了, 整个程序就死锁了. 虽然绝大部分的RPC实现都有这个潜在问题, 但实际出现频率似乎很低, 只要养成不在锁内做RPC的好习惯, 这是完全可以规避的.  Q：bthread会有Channel吗？ 不会。channel代表的是两点间的关系，而很多现实问题是多点的，这个时候使用channel最自然的解决方案就是：有一个角色负责操作某件事情或某个资源，其他线程都通过channel向这个角色发号施令。如果我们在程序中设置N个角色，让它们各司其职，那么程序就能分类有序地运转下去。所以使用channel的潜台词就是把程序划分为不同的角色。channel固然直观，但是有代价：额外的上下文切换。做成任何事情都得等到被调用处被调度，处理，回复，调用处才能继续。这个再怎么优化，再怎么尊重cache locality，也是有明显开销的。另外一个现实是：用channel的代码也不好写。由于业务一致性的限制，一些资源往往被绑定在一起，所以一个角色很可能身兼数职，但它做一件事情时便无法做另一件事情，而事情又有优先级。各种打断、跳出、继续形成的最终代码异常复杂。\n我们需要的往往是buffered channel，扮演的是队列和有序执行的作用，bthread提供了ExecutionQueue，可以完成这个目的。\n","excerpt":"bthread是brpc使用的M:N线程库，目的是在提高程序的并发度的同时，降低编码难度，并在核数日益增多的CPU上提供更好的scalability和cache locality。”M:N“是指M …","ref":"https://brpc.incubator.apache.org/en/docs/bthread/bthread/","title":"bthread"},{"body":"brpc提供了异步接口，所以一个常见的问题是：我应该用异步接口还是bthread？\n短回答：延时不高时你应该先用简单易懂的同步接口，不行的话用异步接口，只有在需要多核并行计算时才用bthread。\n同步或异步 异步即用回调代替阻塞，有阻塞的地方就有回调。虽然在javascript这种语言中回调工作的很好，接受度也非常高，但只要用过，就会发现这和我们需要的回调是两码事，这个区别不是lambda，也不是future，而是javascript是单线程的。javascript的回调放到多线程下可能没有一个能跑过，竞争太多，单线程的同步方法和多线程的同步方法是完全不同的。那是不是服务能搞成类似的形式呢？多个线程，每个都是独立的eventloop。可以，ubaserver就是（注意带a)，但实际效果糟糕，因为阻塞改回调可不简单，当阻塞发生在循环，条件分支，深层子函数中时，改造特别困难，况且很多老代码、第三方代码根本不可能去改造。结果是代码中会出现不可避免的阻塞，导致那个线程中其他回调都被延迟，流量超时，server性能不符合预期。如果你说，”我想把现在的同步代码改造为大量的回调，除了我其他人都看不太懂，并且性能可能更差了”，我猜大部分人不会同意。别被那些鼓吹异步的人迷惑了，他们写的是从头到尾从下到上全异步且不考虑多线程的代码，和你要写的完全是两码事。\nbrpc中的异步和单线程的异步是完全不同的，异步回调会运行在与调用处不同的线程中，你会获得多核扩展性，但代价是你得意识到多线程问题。你可以在回调中阻塞，只要线程够用，对server整体的性能并不会有什么影响。不过异步代码还是很难写的，所以我们提供了组合访问来简化问题，通过组合不同的channel，你可以声明式地执行复杂的访问，而不用太关心其中的细节。\n当然，延时不长，qps不高时，我们更建议使用同步接口，这也是创建bthread的动机：维持同步代码也能提升交互性能。\n判断使用同步或异步：计算qps * latency(in seconds)，如果和cpu核数是同一数量级，就用同步，否则用异步。\n比如：\n qps = 2000，latency = 10ms，计算结果 = 2000 * 0.01s = 20。和常见的32核在同一个数量级，用同步。 qps = 100, latency = 5s, 计算结果 = 100 * 5s = 500。和核数不在同一个数量级，用异步。 qps = 500, latency = 100ms，计算结果 = 500 * 0.1s = 50。基本在同一个数量级，可用同步。如果未来延时继续增长，考虑异步。  这个公式计算的是同时进行的平均请求数（你可以尝试证明一下），和线程数，cpu核数是可比的。当这个值远大于cpu核数时，说明大部分操作并不耗费cpu，而是让大量线程阻塞着，使用异步可以明显节省线程资源（栈占用的内存）。当这个值小于或和cpu核数差不多时，异步能节省的线程资源就很有限了，这时候简单易懂的同步代码更重要。\n异步或bthread 有了bthread这个工具，用户甚至可以自己实现异步。以“半同步”为例，在brpc中用户有多种选择：\n 发起多个异步RPC后挨个Join，这个函数会阻塞直到RPC结束。（这儿是为了和bthread对比，实现中我们建议你使用ParallelChannel，而不是自己Join） 启动多个bthread各自执行同步RPC后挨个join bthreads。  哪种效率更高呢？显然是前者。后者不仅要付出创建bthread的代价，在RPC过程中bthread还被阻塞着，不能用于其他用途。\n如果仅仅是为了并发RPC，别用bthread。\n不过当你需要并行计算时，问题就不同了。使用bthread可以简单地构建树形的并行计算，充分利用多核资源。比如检索过程中有三个环节可以并行处理，你可以建立两个bthread运行两个环节，在原地运行剩下的环节，最后join那两个bthread。过程大致如下：\nbool search() { ... bthread th1, th2; if (bthread_start_background(\u0026amp;th1, NULL, part1, part1_args) != 0) { LOG(ERROR) \u0026lt;\u0026lt; \u0026#34;Fail to create bthread for part1\u0026#34;; return false; } if (bthread_start_background(\u0026amp;th2, NULL, part2, part2_args) != 0) { LOG(ERROR) \u0026lt;\u0026lt; \u0026#34;Fail to create bthread for part2\u0026#34;; return false; } part3(part3_args); bthread_join(th1); bthread_join(th2); return true; } 这么实现的point：\n 你当然可以建立三个bthread分别执行三个部分，最后join它们，但相比这个方法要多耗费一个线程资源。 bthread从建立到执行是有延时的（调度延时），在不是很忙的机器上，这个延时的中位数在3微秒左右，90%在10微秒内，99.99%在30微秒内。这说明两点：  计算时间超过1ms时收益比较明显。如果计算非常简单，几微秒就结束了，用bthread是没有意义的。 尽量让原地运行的部分最慢，那样bthread中的部分即使被延迟了几微秒，最后可能还是会先结束，而消除掉延迟的影响。并且join一个已结束的bthread时会立刻返回，不会有上下文切换开销。    另外当你有类似线程池的需求时，像执行一类job的线程池时，也可以用bthread代替。如果对job的执行顺序有要求，你可以使用基于bthread的ExecutionQueue。\n","excerpt":"brpc提供了异步接口，所以一个常见的问题是：我应该用异步接口还是bthread？\n短回答：延时不高时你应该先用简单易懂的同步接口，不行的话用异步接口，只有在需要多核并行计算时才用bthread。\n同 …","ref":"https://brpc.incubator.apache.org/en/docs/bthread/bthread-or-not/","title":"bthread or not"},{"body":"概述 类似于kylin的ExecMan, ExecutionQueue提供了异步串行执行的功能。ExecutionQueue的相关技术最早使用在RPC中实现多线程向同一个fd写数据. 在r31345之后加入到bthread。 ExecutionQueue 提供了如下基本功能:\n 异步有序执行: 任务在另外一个单独的线程中执行, 并且执行顺序严格和提交顺序一致. Multi Producer: 多个线程可以同时向一个ExecutionQueue提交任务 支持cancel一个已经提交的任务 支持stop 支持高优任务插队  和ExecMan的主要区别:\n ExecutionQueue的任务提交接口是wait-free的, ExecMan依赖了lock, 这意味着当机器整体比较繁忙的时候，使用ExecutionQueue不会因为某个进程被系统强制切换导致所有线程都被阻塞。 ExecutionQueue支持批量处理: 执行线程可以批量处理提交的任务, 获得更好的locality. ExecMan的某个线程处理完某个AsyncClient的AsyncContext之后下一个任务很可能是属于另外一个AsyncClient的AsyncContex, 这时候cpu cache会在不同AsyncClient依赖的资源间进行不停的切换。 ExecutionQueue的处理函数不会被绑定到固定的线程中执行, ExecMan中是根据AsyncClient hash到固定的执行线程，不同的ExecutionQueue之间的任务处理完全独立，当线程数足够多的情况下，所有非空闲的ExecutionQueue都能同时得到调度。同时也意味着当线程数不足的时候，ExecutionQueue无法保证公平性, 当发生这种情况的时候需要动态增加bthread的worker线程来增加整体的处理能力. ExecutionQueue运行线程为bthread, 可以随意的使用一些bthread同步原语而不用担心阻塞pthread的执行. 而在ExecMan里面得尽量避免使用较高概率会导致阻塞的同步原语.  背景 在多核并发编程领域， Message passing作为一种解决竞争的手段得到了比较广泛的应用，它按照业务依赖的资源将逻辑拆分成若干个独立actor，每个actor负责对应资源的维护工作，当一个流程需要修改某个资源的时候， 就转化为一个消息发送给对应actor，这个actor(通常在另外的上下文中)根据命令内容对这个资源进行相应的修改，之后可以选择唤醒调用者(同步)或者提交到下一个actor(异步)的方式进行后续处理。\nExecutionQueue Vs Mutex ExecutionQueue和mutex都可以用来在多线程场景中消除竞争. 相比较使用mutex, 使用ExecutionQueue有着如下几个优点:\n 角色划分比较清晰, 概念理解上比较简单, 实现中无需考虑锁带来的问题(比如死锁) 能保证任务的执行顺序，mutex的唤醒顺序不能得到严格保证. 所有线程各司其职，都能在做有用的事情，不存在等待. 在繁忙、卡顿的情况下能更好的批量执行，整体上获得较高的吞吐.  但是缺点也同样明显:\n 一个流程的代码往往散落在多个地方，代码理解和维护成本高。 为了提高并发度， 一件事情往往会被拆分到多个ExecutionQueue进行流水线处理，这样会导致在多核之间不停的进行切换，会付出额外的调度以及同步cache的开销, 尤其是竞争的临界区非常小的情况下， 这些开销不能忽略. 同时原子的操作多个资源实现会变得复杂, 使用mutex可以同时锁住多个mutex, 用了ExeuctionQueue就需要依赖额外的dispatch queue了。 由于所有操作都是单线程的，某个任务运行慢了就会阻塞同一个ExecutionQueue的其他操作。 并发控制变得复杂，ExecutionQueue可能会由于缓存的任务过多占用过多的内存。  不考虑性能和复杂度，理论上任何系统都可以只使用mutex或者ExecutionQueue来消除竞争. 但是复杂系统的设计上，建议根据不同的场景灵活决定如何使用这两个工具:\n 如果临界区非常小，竞争又不是很激烈，优先选择使用mutex, 之后可以结合contention profiler来判断mutex是否成为瓶颈。 需要有序执行，或者无法消除的激烈竞争但是可以通过批量执行来提高吞吐， 可以选择使用ExecutionQueue。  总之，多线程编程没有万能的模型，需要根据具体的场景，结合丰富的profliling工具，最终在复杂度和性能之间找到合适的平衡。\n特别指出一点，Linux中mutex无竞争的lock/unlock只有需要几条原子指令，在绝大多数场景下的开销都可以忽略不计.\n使用方式 实现执行函数 // Iterate over the given tasks // // Example: // // #include \u0026lt;bthread/execution_queue.h\u0026gt; // // int demo_execute(void* meta, TaskIterator\u0026lt;T\u0026gt;\u0026amp; iter) { // if (iter.is_stopped()) { // // destroy meta and related resources // return 0; // } // for (; iter; ++iter) { // // do_something(meta, *iter) // // or do_something(meta, iter-\u0026gt;a_member_of_T) // } // return 0; // } template \u0026lt;typename T\u0026gt; class TaskIterator; 启动一个ExecutionQueue: // Start a ExecutionQueue. If |options| is NULL, the queue will be created with // default options. // Returns 0 on success, errno otherwise // NOTE: type |T| can be non-POD but must be copy-constructible template \u0026lt;typename T\u0026gt; int execution_queue_start( ExecutionQueueId\u0026lt;T\u0026gt;* id, const ExecutionQueueOptions* options, int (*execute)(void* meta, TaskIterator\u0026lt;T\u0026gt;\u0026amp; iter), void* meta); 创建的返回值是一个64位的id, 相当于ExecutionQueue实例的一个弱引用, 可以wait-free的在O(1)时间内定位一个ExecutionQueue, 你可以到处拷贝这个id， 甚至可以放在RPC中，作为远端资源的定位工具。 你必须保证meta的生命周期，在对应的ExecutionQueue真正停止前不会释放.\n停止一个ExecutionQueue: // Stop the ExecutionQueue. // After this function is called: // - All the following calls to execution_queue_execute would fail immediately. // - The executor will call |execute| with TaskIterator::is_queue_stopped() being // true exactly once when all the pending tasks have been executed, and after // this point it's ok to release the resource referenced by |meta|. // Returns 0 on success, errno othrwise template \u0026lt;typename T\u0026gt; int execution_queue_stop(ExecutionQueueId\u0026lt;T\u0026gt; id); // Wait until the the stop task (Iterator::is_queue_stopped() returns true) has // been executed template \u0026lt;typename T\u0026gt; int execution_queue_join(ExecutionQueueId\u0026lt;T\u0026gt; id); stop和join都可以多次调用， 都会又合理的行为。stop可以随时调用而不用当心线程安全性问题。\n和fd的close类似，如果stop不被调用, 相应的资源会永久泄露。\n安全释放meta的时机: 可以在execute函数中收到iter.is_queue_stopped()==true的任务的时候释放，也可以等到join返回之后释放. 注意不要double-free\n提交任务 struct TaskOptions { TaskOptions(); TaskOptions(bool high_priority, bool in_place_if_possible); // Executor would execute high-priority tasks in the FIFO order but before // all pending normal-priority tasks. // NOTE: We don't guarantee any kind of real-time as there might be tasks still // in process which are uninterruptible. // // Default: false bool high_priority; // If |in_place_if_possible| is true, execution_queue_execute would call // execute immediately instead of starting a bthread if possible // // Note: Running callbacks in place might cause the dead lock issue, you // should be very careful turning this flag on. // // Default: false bool in_place_if_possible; }; const static TaskOptions TASK_OPTIONS_NORMAL = TaskOptions(/*high_priority=*/ false, /*in_place_if_possible=*/ false); const static TaskOptions TASK_OPTIONS_URGENT = TaskOptions(/*high_priority=*/ true, /*in_place_if_possible=*/ false); const static TaskOptions TASK_OPTIONS_INPLACE = TaskOptions(/*high_priority=*/ false, /*in_place_if_possible=*/ true); // Thread-safe and Wait-free. // Execute a task with defaut TaskOptions (normal task); template \u0026lt;typename T\u0026gt; int execution_queue_execute(ExecutionQueueId\u0026lt;T\u0026gt; id, typename butil::add_const_reference\u0026lt;T\u0026gt;::type task); // Thread-safe and Wait-free. // Execute a task with options. e.g // bthread::execution_queue_execute(queue, task, \u0026amp;bthread::TASK_OPTIONS_URGENT) // If |options| is NULL, we will use default options (normal task) // If |handle| is not NULL, we will assign it with the hanlder of this task. template \u0026lt;typename T\u0026gt; int execution_queue_execute(ExecutionQueueId\u0026lt;T\u0026gt; id, typename butil::add_const_reference\u0026lt;T\u0026gt;::type task, const TaskOptions* options); template \u0026lt;typename T\u0026gt; int execution_queue_execute(ExecutionQueueId\u0026lt;T\u0026gt; id, typename butil::add_const_reference\u0026lt;T\u0026gt;::type task, const TaskOptions* options, TaskHandle* handle); high_priority的task之间的执行顺序也会严格按照提交顺序, 这点和ExecMan不同, ExecMan的QueueExecEmergent的AsyncContex执行顺序是undefined. 但是这也意味着你没有办法将任何任务插队到一个high priority的任务之前执行.\n开启inplace_if_possible, 在无竞争的场景中可以省去一次线程调度和cache同步的开销. 但是可能会造成死锁或者递归层数过多(比如不停的ping-pong)的问题，开启前请确定你的代码中不存在这些问题。\n取消一个已提交任务 /// [Thread safe and ABA free] Cancel the corresponding task. // Returns: // -1: The task was executed or h is an invalid handle // 0: Success // 1: The task is executing int execution_queue_cancel(const TaskHandle\u0026amp; h); 返回非0仅仅意味着ExecutionQueue已经将对应的task递给过execute, 真实的逻辑中可能将这个task缓存在另外的容器中，所以这并不意味着逻辑上的task已经结束，你需要在自己的业务上保证这一点.\n","excerpt":"概述 类似于kylin的ExecMan, ExecutionQueue提供了异步串行执行的功能。ExecutionQueue的相关技术最早使用在RPC中实现多线程向同一个fd写数据. 在r31345之 …","ref":"https://brpc.incubator.apache.org/en/docs/bthread/execution-queue/","title":"Execution Queue"},{"body":"本页说明bthread下使用pthread-local可能会导致的问题。bthread-local的使用方法见这里。\nthread-local问题 调用阻塞的bthread函数后，所在的pthread很可能改变，这使pthread_getspecific，gcc __thread和c++11 thread_local变量，pthread_self()等的值变化了，如下代码的行为是不可预计的：\nthread_local SomeObject obj; ... SomeObject* p = \u0026amp;obj; p-\u0026gt;bar(); bthread_usleep(1000); p-\u0026gt;bar(); bthread_usleep之后，该bthread很可能身处不同的pthread，这时p指向了之前pthread的thread_local变量，继续访问p的结果无法预计。这种使用模式往往发生在用户使用线程级变量传递业务变量的情况。为了防止这种情况，应该谨记：\n 不使用线程级变量传递业务数据。这是一种槽糕的设计模式，依赖线程级数据的函数也难以单测。判断是否滥用：如果不使用线程级变量，业务逻辑是否还能正常运行？线程级变量应只用作优化手段，使用过程中不应直接或间接调用任何可能阻塞的bthread函数。比如使用线程级变量的tcmalloc就不会和bthread有任何冲突。 如果一定要（在业务中）使用线程级变量，使用bthread_key_create和bthread_getspecific。  gcc4下的errno问题 gcc4会优化标记为__attribute__((__const__))的函数，这个标记大致指只要参数不变，输出就不会变。所以当一个函数中以相同参数出现多次时，gcc4会合并为一次。比如在我们的系统中errno是内容为*__errno_location()的宏，这个函数的签名是：\n/* Function to get address of global `errno' variable. */ extern int *__errno_location (void) __THROW __attribute__ ((__const__)); 由于此函数被标记为__const__，且没有参数，当你在一个函数中调用多次errno时，可能只有第一次才调用__errno_location()，而之后只是访问其返回的int*。在pthread中这没有问题，因为返回的int*是thread-local的，一个给定的pthread中是不会变化的。但是在bthread中，这是不成立的，因为一个bthread很可能在调用一些函数后跑到另一个pthread去，如果gcc4做了类似的优化，即一个函数内所有的errno都替换为第一次调用返回的int*，这中间bthread又切换了pthread，那么可能会访问之前pthread的errno，从而造成未定义行为。\n比如下文是一种errno的使用场景：\nUse errno ... (original pthread) bthread functions that may switch to another pthread. Use errno ... (another pthread) 我们期望看到的行为：\nUse *__errno_location() ... - the thread-local errno of original pthread bthread may switch another pthread ... Use *__errno_location() ... - the thread-local errno of another pthread 使用gcc4时的实际行为：\nint* p= __errno_location(); Use *p ... - the thread-local errno of original pthread bthread context switches ... Use *p ... - still the errno of original pthread, undefined behavior!! 严格地说这个问题不是gcc4导致的，而是glibc给__errno_location的签名不够准确，一个返回thread-local指针的函数依赖于段寄存器（TLS的一般实现方式），这怎么能算const呢？由于我们还未找到覆盖__errno_location的方法，所以这个问题目前实际的解决方法是：\n务必在直接或间接使用bthread的项目的gcc编译选项中添加-D__const__=，即把__const__定义为空，避免gcc4做相关优化。\n把__const__定义为空对程序其他部分的影响几乎为0。另外如果你没有直接使用errno（即你的项目中没有出现errno），或使用的是gcc 3.4，即使没有定义-D__const__=，程序的正确性也不会受影响，但为了防止未来可能的问题，我们强烈建议加上。\n需要说明的是，和errno类似，pthread_self也有类似的问题，不过一般pthread_self除了打日志没有其他用途，影响面较小，在-D__const__=后pthread_self也会正常。\n","excerpt":"本页说明bthread下使用pthread-local可能会导致的问题。bthread-local的使用方法见这里。\nthread-local问题 调用阻塞的bthread函数后，所在的pthread …","ref":"https://brpc.incubator.apache.org/en/docs/bthread/thread-local/","title":"thread-local"},{"body":"This topic describes the TLS security capability of MOSN.\nCertificate solution MOSN provides a certificate issuance solution based on Istio Citadel. It configures Sidecar certificates by using the Secret Discovery Service (SDS) of the Istio community. Dynamic certificate discovery and hot updates are supported. To provide advanced security capabilities, MOSN obtains certificates by connecting to the internal Key Management Service (KMS) system, without relying on the self-certificate issuance capability of Citadel. MOSN also supports certificate caching, pushing, and updates.\nThe following figure shows the architecture of the MOSN\u0026rsquo;s certificate solution.\nRoles of the components are described as follows:\n Pilot: sends policies and SDS configurations (not shown in the figure for brevity). Citadel: acts as the Certificate Provider as well as the MCP Server to provide resources to the Citadel Agent such as Pods and custom resources (CRs). Citadel Agent: provides SDS Server services and sends certificates and CRs for MOSN, DB Sidecar, and Security Sidecar. KMS: issues certificates.  Certificate acquisition procedure After having a general idea about the overall architecture, we can break down the certificate acquisition procedure for the sidecar as shown in the following figure.\nEach step in the figure is described as follows:\n Citadel synchronizes Pod and CR information with the Citadel Agent (nodeagent) through the Mesh Configuration Protocol (MCP). This avoids overload of the API Server caused by direct requests from the Citadel Agent to the API Server. MOSN sends an SDS request to the Citadel Agent by using the Unix Domain Socket. The Citadel Agent performs tamper-proof verification and extracts the appkey. The Citadel Agent uses the appkey to request Citadel to issue a certificate. Citadel checks whether a certificate has been cached. If a valid certificate is cached, Citadel directly returns the cached certificate. If no certificate is cached, Citadel constructs a certificate issuance request and requests the KMS to issue a certificate. The KMS returns a certificate to Citadel. The KMS also generates certificate expiration and rotation notifications. After receiving the certificate, Citadel transfers it to the corresponding Citadel Agent. After receiving the certificate, the Citadel Agent caches it in memory and sends it to MOSN.  ","excerpt":"This topic describes the TLS security capability of MOSN.\nCertificate solution MOSN provides a …","ref":"https://brpc.incubator.apache.org/en/docs/concept/tls/","title":"TLS connections"},{"body":"Sidecar O\u0026amp;M is always challenging for a service mesh, while sidecar upgrades are common at the data plane. This topic describes how to upgrade the sidecar (MOSN) without affecting the business and how to migrate existing persistent connections.\nBackground This topic describes why and how MOSN supports hot upgrade. For details about the basic concepts of hot upgrade, see the NGINX vs Envoy vs MOSN hot upgrade.\nFirst, why don\u0026rsquo;t NGINX and Envoy require a connection-lossless migration solution like MOSN does? This depends on their business scenarios. NGINX and Envoy mainly support the HTTP1 and HTTP2 protocols. The connection: Close request/response header in HTTP1 and GOAWAY frame in HTTP2 allow a client to actively close a connection and establish a new one to a new process. However, common multiplexing protocols such as Dubbo and SOFARPC do not provide control frames, and a request will fail if the connection to the old process is closed.\nA common upgrade approach is to: cut off the application\u0026rsquo;s traffic, for example, by unpublishing the service; upgrade MOSN when no new request is received; and then publish the service again. This process takes a rather long time, and the service is unavailable during this period of time. In addition, the application usage also needs to be considered. Achieving a balance between service availability and the upgrade in a large-scale scenario is difficult. To adapt to business scenarios of MOSN, a persistent-connection migration solution is developed to migrate persistent connections to new processes. The entire procedure is transparent to the client, and no connection needs to be re-established, implementing a request-lossless hot upgrade.\nNormal procedure  A client sends a request to MOSN. MOSN forwards the request to a server. The server returns a response to MOSN. MOSN returns the response to the client.  The preceding figure briefly shows a normal request procedure. Next, we need to migrate TCP1 connections between the client and MOSN. TCP2 connections between MOSN and the server do not need to be migrated, because the server accessed by MOSN is selected through load balancing. The connection/disconnection with the server is not our concern.\nHot upgrade procedure Trigger conditions We can trigger a hot upgrade through either of the following methods:\n Register a SIGHUP event listener with MOSN, and send a SIGHUP signal to the MOSN process to call ForkExec to generate a new MOSN process. Directly start a new MOSN process.  Why do we provide two methods? In the beginning, only the first method was supported, which is used by NGINX and Envoy. In this method, we can replace the old MOSN binary file in a virtual machine or container for an upgrade. However, our scenarios require cross-container upgrades. We need to start a new container (a new MOSN process) to implement a hot upgrade. That is why the second method is provided. Cross-container upgrades also require support from operators, but this will not be discussed in detail here.\nInteraction procedure The old MOSN process will start a goroutine to run the ReconfigureHandler() function to listen to a domain socket (reconfig.sock) in the last stage. This operation enables the new MOSN process to detect whether an old MOSN process exists.\nfunc ReconfigureHandler() { l, err := net.Listen(\u0026#34;unix\u0026#34;, types.ReconfigureDomainSocket) for { uc, err := ul.AcceptUnix() _, err = uc.Write([]byte{0}) reconfigure(false) } } Both hot upgrade triggering methods start a new MOSN process at last. The new MOSN process will then successively call the GetInheritListeners() and isReconfigure() functions to verify whether an old MOSN process exists (whether the reconfig.sock listener exists). If yes, MOSN starts the migration procedure; otherwise, MOSN starts a normal startup procedure.\n// The core procedure is kept. func GetInheritListeners() ([]net.Listener, net.Conn, error) { if !isReconfigure() { return nil, nil, nil } l, err := net.Listen(\u0026#34;unix\u0026#34;, types.TransferListenDomainSocket) uc, err := ul.AcceptUnix() _, oobn, _, _, err := uc.ReadMsgUnix(buf, oob) file := os.NewFile(fd, \u0026#34;\u0026#34;) fileListener, err := net.FileListener(file) return listeners, uc, nil } If the migration procedure starts, the new MOSN process will listen to a new domain socket (listen.sock). This ensures that the old MOSN process can transfer the listen FD to the new MOSN process. sendMsg and recvMsg are used to transfer the listen FD. After receiving the listen FD, the new MOSN process calls the net.FileListener() function to generate a listener. In this case, the new and old MOSN processes have the same listen socket.\n// FileListener returns a copy of the network listener corresponding // to the open file f. // It is the caller\u0026#39;s responsibility to close ln when finished. // Closing ln does not affect f, and closing f does not affect ln. func FileListener(f *os.File) (ln Listener, err error) { ln, err = fileListener(f) if err != nil { err = \u0026amp;OpError{Op: \u0026#34;file\u0026#34;, Net: \u0026#34;file+net\u0026#34;, Source: nil, Addr: fileAddr(f.Name()), Err: err} } return } The migration procedure of MOSN is different from that of NGINX. In NGINX, after the forking is done, the child process automatically inherits the listen FD. However, MOSN starts a new process that is independent from the old one, without a parent-child relationship. Therefore, sendMsg is required for transferring the listen FD.\nA total of two domain sockets are used to start the migration and transfer the listen FD.\n reconfig.sock is the old MOSN listener used by the new MOSN process to verify whether an old MOSN process exists. listen.sock is the new MOSN listener used by the old MOSN process to transfer the listen FD.  These two sockets are actually interchangeable. For example, reconfig.sock can also be used for transferring the listen FD. These two sockets are used for some historical reasons. They can be merged into one later on, to make the code simpler and easier to read.\nLet us take a look at the handling procedure of the old MOSN process. After receiving the notification from the new MOSN process, the old MOSN process starts the reconfigure(false) procedure. It first calls sendInheritListeners() to transfer the listen FD to the new MOSN process as described above, and then calls WaitConnectionsDone() to migrate existing persistent connections.\n// The core procedure is kept. func reconfigure(start bool) { if start { startNewMosn() return } // transfer listen fd  if notify, err = sendInheritListeners(); err != nil { return } // Wait for all connections to be finished  WaitConnectionsDone(GracefulTimeout) os.Exit(0) } After receiving the listen FD, the new MOSN process starts the migration process based on the configurations. Then the new MOSN process starts a goroutine to run TransferServer() to listen to a new domain socket (conn.sock), for receiving persistent connections from the old MOSN process subsequently. The migration function is transferHandler().\nfunc TransferServer(handler types.ConnectionHandler) { l, err := net.Listen(\u0026#34;unix\u0026#34;, types.TransferConnDomainSocket) utils.GoWithRecover(func() { for { c, err := l.Accept() go transferHandler(c, handler, \u0026amp;transferMap) } }, nil) } The old MOSN process starts persistent-connection migration through transferRead() and transferWrite(). This is analyzed as follows.\nPersistent-connection migration procedure First, let us take a look at the migration procedure of a new request.\n A client sends a request to MOSN. MOSN (the old MOSN process) sends the FD and connection status data of TCP1 to New MOSN (the new MOSN process). New MOSN receives the FD and request data, creates a new connection structure, and sends the connection ID to the MOSN. At this time, New MOSN has a copy of the TCP1 connection. New MOSN selects a new server by using the load balancer, establishes a TCP3 connection, and forwards the request to the server. The server returns a response to New MOSN. New MOSN returns a response to the client based on the copy of TCP1 connection transferred from MOSN.  In the original WaitConnectionsDone() function, s.stopChan has been disabled. In ReadLoop of the connection, a [TransferTimeout, 2 * TransferTimeout] random time interval will be set for the migration procedure. The random interval is intended to discretize the migration time for TCP connections of each client, to ensure smooth migration.\nfunc (c *connection) startReadLoop() { var transferTime time.Time for { select { case \u0026lt;-c.stopChan: if transferTime.IsZero() { if c.transferCallbacks != nil \u0026amp;\u0026amp; c.transferCallbacks() { randTime := time.Duration(rand.Intn(int(TransferTimeout.Nanoseconds()))) transferTime = time.Now().Add(TransferTimeout).Add(randTime) log.DefaultLogger.Infof(\u0026#34;[network] [read loop] transferTime: Wait %d Second\u0026#34;, (TransferTimeout+randTime)/1e9) } else { // set a long time, not transfer connection, wait mosn exit.  transferTime = time.Now().Add(10 * TransferTimeout) log.DefaultLogger.Infof(\u0026#34;[network] [read loop] not support transfer connection, Connection = %d, Local Address = %+v, Remote Address = %+v\u0026#34;, c.id, c.rawConnection.LocalAddr(), c.RemoteAddr()) } } else { if transferTime.Before(time.Now()) { c.transfer() return } } After one random interval elapses, the c.transfer() function is called. c.notifyTransfer() is used for suspending write operations. No write operation is allowed during migration of read operations, because data confusion will occur if write operations are performed simultaneously in the old and new MOSN processes.\nfunc (c *connection) transfer() { c.notifyTransfer() id, _ := transferRead(c) c.transferWrite(id) } Then the transferRead() function is called to transfer the FD and status data of a connection to New MOSN through conn.sock. Similar to migrating the listen FD, New MOSN returns an ID after successful processing. This ID identifies the new connection established by New MOSN and will be used later.\n// old mosn transfer readloop func transferRead(c *connection) (uint64, error) { unixConn, err := net.Dial(\u0026#34;unix\u0026#34;, types.TransferConnDomainSocket) file, tlsConn, err := transferGetFile(c) uc := unixConn.(*net.UnixConn) // send type and TCP FD  err = transferSendType(uc, file) // send header + buffer + TLS  err = transferReadSendData(uc, tlsConn, c.readBuffer, log.DefaultLogger) // recv ID  id := transferRecvID(uc) return id, nil } We constructed a simple read transfer protocol, which mainly involves the length of the TCP raw data, the length of the TLS data, the TCP raw data, and the TLS data.\n/** * transfer read protocol * header (8 bytes) + (readBuffer data) + TLS * * 0 4 8 * +-----+-----+-----+-----+-----+-----+-----+-----+ * | data length | TLS length | * +-----+-----+-----+-----+-----+-----+-----+-----+ * | data | * +-----+-----+-----+-----+-----+-----+-----+-----+ * | TLS | * +-----+-----+-----+-----+-----+-----+-----+-----+ * Now, let us take a look at the handling procedure of the new MOSN process. After receiving migration requests, the new MOSN process starts a goroutine for each migration request to run the transferHandler() function. The function distinguishes read and write transfer requests based on the protocol read. Read transfer is described first. The new MOSN process calls transferNewConn to generate a new connection structure based on the FD and packets transferred from the old MOSN process. Then the new MOSN process transfers the new connection ID to the old MOSN process.\nThe new MOSN process starts to read data from the new TCP connection, and proceeds with a normal business request procedure.\nfunc transferHandler(c net.Conn, handler types.ConnectionHandler, transferMap *sync.Map) { // recv type  conn, err := transferRecvType(uc) if err != nil { log.DefaultLogger.Errorf(\u0026#34;[network] [transfer] [handler] transferRecvType error :%v\u0026#34;, err) return } if conn != nil { // transfer read  // recv header + buffer  dataBuf, tlsBuf, err := transferReadRecvData(uc) if err != nil { log.DefaultLogger.Errorf(\u0026#34;[network] [transfer] [handler] transferRecvData error :%v\u0026#34;, err) return } connection := transferNewConn(conn, dataBuf, tlsBuf, handler, transferMap) if connection != nil { transferSendID(uc, connection.id) } else { transferSendID(uc, transferErr) } } else { // transfer write  // recv header + buffer  id, buf, err := transferWriteRecvData(uc) if err != nil { log.DefaultLogger.Errorf(\u0026#34;[network] [transfer] [handler] transferRecvData error :%v\u0026#34;, err) } connection := transferFindConnection(transferMap, uint64(id)) if connection == nil { log.DefaultLogger.Errorf(\u0026#34;[network] [transfer] [handler] transferFindConnection failed, id = %d\u0026#34;, id) return } err = transferWriteBuffer(connection, buf) if err != nil { log.DefaultLogger.Errorf(\u0026#34;[network] [transfer] [handler] transferWriteBuffer error :%v\u0026#34;, err) return } } } Then the new MOSN process replaces the old MOSN process, to read data from the TCP1 connection and process the data. For the new request, the entire migration procedure is completed.\nResidual-response migration procedure Why is a residual-response migration procedure required? Because of the multiplexing protocol, during the previous migration of read connections, residual responses for TCP2 connections are waiting to be returned to the client. The data may go out of order if both the old and new MOSN processes simultaneously write data to TCP1 connections. Therefore, we want the new MOSN process to uniformly handle the residual responses for TCP2 connections.\n The server returns a residual response to MOSN. MOSN transfers the connection ID and response data previously obtained from New MOSN back to New MOSN through domain socket (conn.sock). New MOSN queries the TCP1 connection based on the ID and returns the response to the client.  After the transferRead() (the read transfer) ends, the transferWrite() (the write transfer) starts. In this stage, packets to be written and the connection ID previously obtained from New MOSN are sent to New MOSN.\n// old mosn transfer writeloop func transferWrite(c *connection, id uint64) error { unixConn, err := net.Dial(\u0026#34;unix\u0026#34;, types.TransferConnDomainSocket) uc := unixConn.(*net.UnixConn) err = transferSendType(uc, nil) // build net.Buffers to IoBuffer  buf := transferBuildIoBuffer(c) // send header + buffer  err = transferWriteSendData(uc, int(id), buf) if err != nil { log.DefaultLogger.Errorf(\u0026#34;[network] [transfer] [write] transferWrite failed: %v\u0026#34;, err) return err } return nil } We constructed a simple write transfer protocol, which mainly involves the length of the TCP raw data, the connection ID, and the TCP raw data.\n/* * transfer write protocol * header (8 bytes) + (writeBuffer data) * * 0 4 8 * +-----+-----+-----+-----+-----+-----+-----+-----+ * | data length | connection ID | * +-----+-----+-----+-----+-----+-----+-----+-----+ * | data | * +-----+-----+-----+-----+-----+-----+-----+-----+ * **/ The new MOSN process calls the transferHandler() function to identify the write transfer protocol. Then the new MOSN process calls the transferFindConnection() function to locate the TCP1 connection based on the connection ID, and directly write the data.\nNote that new requests are now forwarded to the server through TCP3 connections, and only responses to previous requests will be returned through TCP2 connections. If no response is returned within 2 * TransferTimeout during the entire migration, the requests will time out and fail.\nConnection status data During the connection migration, both the TCP FD and the connection status are migrated so that the new MOSN process knows how to initialize the new connection.\nThe following status data is involved:\nRead buffer\nThe data that has been read from TCP but has not been processed at the application layer during migration.\nWrite data\nThe response data received by MOSN after migration.\nTLS status data migration\nIn case of a TLS encrypted request, the following TLS status data must to be migrated:\n Encryption key Sequence Read buffer data (encrypted/unencrypted) Cipher type TLS version  type TransferTLSInfo struct { Vers uint16 CipherSuite uint16 MasterSecret []byte ClientRandom []byte ServerRandom []byte InSeq [8]byte OutSeq [8]byte RawInput []byte Input []byte } Summary FD migration is a common operation for persistent connection migration, and can be performed through either sendMsg or connection repair.\nThe most challenging part in the entire procedure is the migration of the application layer data. The general idea is to migrate all data structures of the application layer to the new process, for example, structures such as protocol headers that have been read. However, this increases the migration complexity, because each protocol needs to be handled separately.\nTo address this problem, MOSN moves migration to the I/O layer, regardless of the protocols used at the application layer. The original TCP packets are migrated, and then the new MOSN process encodes/decodes the packets to assemble the header and other structures. This is a standard procedure that enables migration without parsing protocols. This migration framework can automatically support any stateless protocols.\nYou may doubt about the residual-response migration procedure. Why don\u0026rsquo;t we start migration after all responses are returned? This procedure seems to be unnecessary. The reason is that when we use a multiplexing protocol, requests are being sent all the time. You cannot always find a time point when all responses are returned.\nFeedback For more information about discussions on this issue, go to Github Issue at MOSN smooth upgrade problem #866.\n","excerpt":"Sidecar O\u0026amp;M is always challenging for a service mesh, while sidecar upgrades are common at the …","ref":"https://brpc.incubator.apache.org/en/docs/concept/smooth-upgrade/","title":"MOSN hot upgrade"},{"body":"","excerpt":"","ref":"https://brpc.incubator.apache.org/en/docs/contribute/","title":"Document contribution"},{"body":"The official bRPC documentation.\n","excerpt":"The official bRPC documentation.","ref":"https://brpc.incubator.apache.org/en/docs/","title":"Docs"},{"body":"","excerpt":"","ref":"https://brpc.incubator.apache.org/en/blog/releases/","title":"Release"},{"body":"We are happy to announce the release of MOSN v0.23.0\nv0.23.0 New Features  Add new networkfilter:grpc. A grpc server can be extended by networkfilter and implemented in the MOSN, reuse MOSN capabilities. @nejisama @zhenjunMa Add a new extended function for traversal calls in the StreamFilterChain. @wangfakang Add HTTP 403 status code mapping in the bolt protocol. @pxzero Add the ability to shutdown the upstream connections. @nejisama  Optimization  Optimize the networkfilter configuration parsed. @nejisama Support extend proxy config by protocol, optimize the proxy configuration parse timing. @nejisama Add tls conenction\u0026rsquo;s certificate cache, reduce the memory usage. @nejisama Optimize Quick Start Sample. @nobodyiam Reuse context when router handler check available. @alpha-baby Modify the NewSubsetLoadBalancer\u0026rsquo;s input parameter to interface instead of the determined structure. @alpha-baby Add an example of using so plugin to implement a protocol. @yichouchou   Optimize the method of get environment variable GOPATH in the MAKEFILE. @bincherry Support darwin \u0026amp; aarch64 architecture. @nejisama   Optimize the logger file flag. @taoyuanyuan  Bug fixes  Fix the bug of HTTP1 URL encoding. @morefreeze Fix the bug of HTTP1 URL case-sensitive process. @GLYASAI Fix the bug of memory leak in error handling when the tls cipher suite is SM4. @william-zk  ","excerpt":"We are happy to announce the release of MOSN v0.23.0\nv0.23.0 New Features  Add new …","ref":"https://brpc.incubator.apache.org/en/blog/releases/v0.23.0/","title":"Announcing MOSN v0.23.0"},{"body":"We are happy to announce the release of MOSN v0.22.0\nv0.22.0 New Features  Add Wasm extension framework @antJack Add x-bolt sub-protocol to allow wasm-based codec for XProtocol @zonghaishang Support fallback through SO_ORIGINAL_DST when protocol auto-matching got failed @antJack Support Go Plugin mode for XProtocol @fdingiit Support for network extension @wangfakang Update to Istio xDS v3 API @champly Branch: istio-1.7.7  Optimization  Remove redundant file path clean when resolving StreamFilter configs @eliasyaoyc Allow setting a unified callback handler for the StreamFilterChain @antJack Support multi-stage execution and remove state lock for the FeatureGate @nejisama Add trace support for HTTP2 @OrezzerO  Refactoring  Add StageManger to divide the bootstrap procedure of MOSN into four configurable stages @nejisama Unify the type definitions of XProtocol and move into mosn.io/api package @fdingiit Add GetTimeout method for XProtocol to replace the variable getter @nejisama  Bug fixes  Fix concurrent read and write for RequestInfo in Proxy @nejisama Fix the safety bug when forwarding the request URI @antJack Fix concurrent slice read and write for Router configurations when doing persistence @nejisama  ","excerpt":"We are happy to announce the release of MOSN v0.22.0\nv0.22.0 New Features  Add Wasm extension …","ref":"https://brpc.incubator.apache.org/en/blog/releases/v0.22.0/","title":"Announcing MOSN v0.22.0"},{"body":"We are happy to announce the release of MOSN v0.21.0, congratulations to Zechao Zheng(@CodingSinger) on becoming a MOSN Committer and thanks for his contribution to the MOSN community.\nv0.21.0 Optimization  Upgrade sentinel version to v1.0.2 @ansiz Shrink the read buffer of tls when read timeout, reduce tls memory consumption @cch123 Add comments and simplify the implementation of the xprotocol protocol connpool @cch123 Update the mosn registry version @cadeeper @cch123  Refactoring  Optimize header matching logic when routing, support general RPC routing matching implementation @nejisama Delete some of the original constants and add constants used to describe the mechanism of variables @nejisama Refactor flow control module, support custom callback extension, realize the ability to customize filter conditions and modify context information, etc @ansiz  Bug fixes  Fix metrics statistics error when request is abnormal @cch123 Fix the bug that the URL is not escaping before forwarding HTTP request @antJack Fix the variable injection errors in HTTP protocol, Fix the bug that routing rewrite is not supported in the HTTP2 protocol @nejisama  New Features  Support Domain-Specific Language route implementation @CodingSinger StreamFilter supports the dynamic link libraries written in Go @CodingSinger VirtualHost supports per_filter_config configuration in routing configuration @machine3 Xprotocol supports dubbo thrift protocol @cadeeper  ","excerpt":"We are happy to announce the release of MOSN v0.21.0, congratulations to Zechao Zheng(@CodingSinger) …","ref":"https://brpc.incubator.apache.org/en/blog/releases/v0.21.0/","title":"Announcing MOSN v0.21.0"},{"body":"We are happy to announce the release of MOSN v0.20.0, congratulations to Huang Runhao(@GLYASAI) on becoming a MOSN Committer and thanks for his contribution to the MOSN community.\nv0.20.0 Optimization  Add UDS address prefix check before UDS resolution when TCP address resolution fails @wangfakang Optimized the retrial interval for connection pool acquisition @nejisama Add global switch for write loop mode @nejisama Optimize auto protocol matching and add test cases @taoyuanyuan Replace the headers with more efficient variables @CodingSinger Pool the writeBufferChan timer to reduce overhead @cch123 Add MOSN failure detail info into TraceLog @nejisama New read done channel in HTTP protocol processing @alpha-baby Enhance logger rotator @nejisama  Refactoring  Upgrade to golang 1.14.13 @nejisama Refactor router chain extension mode to the route handler extension mode, support different router handler configuration @nejisama Refactor MOSN extended configuration, support load config according to configuration order @nejisama  Bug fixes  Fix the bug no provider available occurred after dubbo2.7.3 @cadeeper Fix the bug that UDS connections were treated as TCP connections in netpoll mode @wangfakang Fix the problem that the HTTP Header cannot be obtained correctly when it is set to an empty value @ianwoolf  New Features  Support old Mosn transfer configuration to new Mosn through UDS to solve the issue that Mosn in XDS mode cannot be smoothly upgraded @alpha-baby Automatic protocol identification supports the identification of XProtocol @cadeeper Support configuration of the keepalive parameters for XProtocol @cch123 Support more detailed time tracking @nejisama Support metrics lazy registration to optimize metrics memory when number of service in cluster is too large @champly Add setter function for default XProtocol multiplex connection pool size @cch123 Support netpoll @cch123 Support broadcast @dengqian Support get tls configurations from LDS response @wZH-CN Add ACK response for SDS @wZH-CN  ","excerpt":"We are happy to announce the release of MOSN v0.20.0, congratulations to Huang Runhao(@GLYASAI) on …","ref":"https://brpc.incubator.apache.org/en/blog/releases/v0.20.0/","title":"Announcing MOSN v0.20.0"},{"body":"We are happy to announce the release of MOSN v0.19.0.\nv0.19.0 Optimization  Use the latest TLS memory optimization scheme @cch123 Proxy log optimization to reduce memory escape @taoyuanyuan Increase the maximum number of connections limit @champly When AccessLog fails to obtain variables, use \u0026ldquo;-\u0026rdquo; instead @champly MaxProcs supports configuring automatic recognition based on CPU usage limits @champly Allow specifying network for cluster @champly  Refactoring  Refactored the StreamFilter framework. The network filter can reuse the stream filter framework @antJack  Bug fixes  Fix HTTP Trace get URL error @wzshiming Fix the ConnectTimeout parameter of xDS cluster is not converted @dengqian Fix the upstreamHostGetter method gets the wrong hostname @dengqian Fix tcp proxy close the connection abnormally @dengqian Fix the lack of default configuration of mixer filter, resulting in a nil pointer reference @glyasai Fix HTTP2 direct response not setting Content-length correctly @wangfakang Fix the nil pointer reference in getAPISourceEndpoint @dylandee Fix memory increase caused by too many Timer applications when Write is piled up @champly Fix the problem of missing stats when Dubbo Filter receives an illegal response @champly  ","excerpt":"We are happy to announce the release of MOSN v0.19.0.\nv0.19.0 Optimization  Use the latest TLS …","ref":"https://brpc.incubator.apache.org/en/blog/releases/v0.19.0/","title":"Announcing MOSN v0.19.0"},{"body":"We are happy to announce the release of MOSN v0.18.0.\nv0.18.0 New Features  Add MOSN configure extension @nejisama Add MOSN configuration tool mosn/configure, improve user configure experience @cch123  Optimization  Avoid copying http response body @wangfakang Upgrade github.com/TarsCloud/TarsGo package, to v1.1.4 @champly Add test for various connpool @cch123 Use sync.Pool to reduce memory cost by TLS connection outBuf @cch123 Reduce xprotocol lock area @cch123 Remove useless parameter of network.NewClientConnection method, remove ALPN detection in Dispatch method of struct streamConn @nejisama Add TerminateStream API to StreamReceiverFilterHandler, with which stream can be reset during handling @nejisama Add client TLS fallback @nejisama Fix TLS HashValue in host @nejisama Fix disable_log admin api typo @nejisama  Bug fixes  Fix go mod tidy failing @champly Fix ResourceExhausted: grpc: received message larger than max when MOSN receive \u0026gt; 4M XDS messages @champly Fix fault tolerance unit-test @wangfakang Fix MOSN reconfig fails when MOSNConfig.servers[].listeners[].bind_port is false @alpha-baby Set timeout for local write buffer send, avoid goroutine leak @cch123 Fix deadloop when TLS timeout @nejisama Fix data isn\u0026rsquo;t modified by SetData method in dubbo.Frame struct @lxd5866  ","excerpt":"We are happy to announce the release of MOSN v0.18.0.\nv0.18.0 New Features  Add MOSN configure …","ref":"https://brpc.incubator.apache.org/en/blog/releases/v0.18.0/","title":"Announcing MOSN v0.18.0"},{"body":"We are happy to announce the release of MOSN v0.17.0.\nv0.17.0 New Features  Add header max size configuration option. @wangfakang Add protocol impement choice whether need workerpool mode. And support workerpool mode concurrent configuration. @cch123 Add UDS feature for listener. @CodingSinger Add dubbo protocol use xDS httproute config filter. @champly  Optimization  Optimiza http situation buffer malloc. @wangfakang Optimize RWMutex for SDS StreamClient. @nejisama Update hessian2 v1.7.0 lib. @cch123 Modify NewStream interface, use callback replace direct. @cch123 Refactor XProtocol connect pool, support pingpong mode, mutiplex mode and bind mode. @cch123 Optimize XProtocol mutiplex mode, support Host max connect configuration. @cch123 Optimize route regex config avoid dump unuse config. @wangfakang  Bug fixes  Fix README ant logo invalid address. @wangfakang Fix header override content when set a longer header to request header. @cch123 Fix Dubbo protocol analysis attachment maybe panic. @champly  ","excerpt":"We are happy to announce the release of MOSN v0.17.0.\nv0.17.0 New Features  Add header max size …","ref":"https://brpc.incubator.apache.org/en/blog/releases/v0.17.0/","title":"Announcing MOSN v0.17.0"},{"body":"We are happy to announce the release of MOSN v0.16.0.\nv0.16.0 Optimization  Logger Roller supports the custom Roller. @wenxuwan Add a SendHijackReplyWithBody API for streamFilter. @wenxuwan The configuration adds option of turning off the smooth upgrade. If the smooth upgrade is turned off, different instances of MOSN can be started on the same machine. @cch123+ Optimize the MOSN integration test framework and add more unit test cases. @nejisama @wangfakang @taoyuanyuan DirectResponse route configuration supports the update mode of XDS. @wangfakang Add a new field of TLSContext for clusterManager configuration. @nejisama  Bug fixes  Fix the bug that UDP connection timeout during the smooth upgrade will cause an endless loop. @dengqian Fix the bug that call DirectResponse in the SendFilter will cause an endless loop. @taoyuanyuan Fix concurrency conflicts in HTTP2 stream counting. @wenxuwan Fix the bug that UDP connection read timeout cause data loss. @dengqian Fix the bug that the response StatusCode cannot be recorded correctly due to the loss of the protocol flag when doing a retry. @dengqian Fix the protocol boltv2 decode error. @nejisama Fix the bug that listener cannot be restarted automatically when listener panic. @alpha-baby Fix the bug that NoCache flag is invalid in variable. @wangfakang Fix concurrency conflicts in SDS reconnect. @nejisama  ","excerpt":"We are happy to announce the release of MOSN v0.16.0.\nv0.16.0 Optimization  Logger Roller supports …","ref":"https://brpc.incubator.apache.org/en/blog/releases/v0.16.0/","title":"Announcing MOSN v0.16.0"},{"body":"We are happy to announce the release of MOSN v0.15.0, congratulations to Deng Qian (@dengqian) on becoming a MOSN Committer and thanks for her contribution to the MOSN community.\nv0.15.0 New Features  Routing Path Rewrite supports configuring the content of Rewrite by regular expression @liangyuanpeng Configure new fields: Extended configuration fields, you can start the configuration by extending the configuration fields; Dubbo service discovery configuration via extended configuration fields @cch123 New DSL feature for easy control of request processing behavior @wangfakang Extended implementation of StreamFilter with new traffic mirroring function @champly Listener configuration adds UDP support @dengqian Configuration format support YAML format parsing @GLYASAI Routing support for HTTP redirect configuration @knight42  Optimization  Istio\u0026rsquo;s stats filter for personalizing metrics based on matching criteria @wzshiming Metrics configuration support to configure the output percentage of the Histogram @champly StreamFilter New state for aborting requests directly and not responding to clients @taoyuanyuan XProtocol hijack response support carry body @champly Apache SkyWalking upgrade to version 0.5.0 arugal Upstream Connection TLS State Determination Modification to support the determination of whether a connection needs to be re-established via a TLS-configured Hash @nejisama Optimize DNS cache logic to prevent DNS flooding issues that can be caused when DNS fails @wangfakang  Bug fixes  Fix the bug that XProtocol protocols determine protocol errors in scenarios with multiple protocols when TLS encryption is enabled @nejisama Fix bug in AccessLog where variables of prefix match type don\u0026rsquo;t work @dengqian Fix bug where Listener configuration parsing is not handled correctly @nejisama Fix Router/Cluster bug that fails to save when the Name field contains a path separator in the file persistence configuration type @nejisama  ","excerpt":"We are happy to announce the release of MOSN v0.15.0, congratulations to Deng Qian (@dengqian) on …","ref":"https://brpc.incubator.apache.org/en/blog/releases/v0.15.0/","title":"Announcing MOSN v0.15.0"},{"body":"We are happy to announce the release of MOSN v0.14.0. Congratulations to Changyu Yao (@trainyao) for becoming a MOSN Committer and thank him for his contribution to the MOSN community.\nv0.14.0 New Features  Support for Istio 1.5.X @wangfakang @trainyao @champly  go-control-plane upgrade to version 0.9.4 xDS support for ACK, new Metrics for xDS. Istio sourceLabels filtering support probe interface with pilot-agent support support for more startup parameters, adapting to Istio agent startup scenarios gzip, strict-dns, original-dst support for xDS updates. Remove Xproxy Logic   Maglev Load Balancing Algorithm Support @trainyao New connection pool implementation for supporting message class requests @cch123 New Metrics for TLS Connection Switching @nejisama Metrics for adding HTTP StatusCode @dengqian Add Metrics Admin API output @dengqian New interface to query the number of current requests for proxy @zonghaishang Support for HostRewrite Header @liangyuanpeng  Optimization  Upgrade tars dependencies to fix compilation issues with higher versions of Golang @wangfakang xDS Configuration Analysis Upgrade Adaptation Istio 1.5.x @wangfakang Optimize log output from proxy @wenxuwan DNS Cache default time changed to 15s @wangfakang HTTP Parameter Route Matching Optimization @wangfakang Upgrade the fasthttp library @wangfakang Optimizing Dubbo Request Forwarding Encoding @zonghaishang Request max body configurable for HTTP support @wangfakang  Bug fixes  Fix Dubbo Decode bug that fails to parse attachment @champly Fix bug where streams could be created before HTTP2 connection is established @dunjut Fix HTTP2 Handling Trailer null pointer exception @taoyuanyuan  Fix bug where HTTP request headers are not standardized by default @nejisama Fix panic exceptions caused by disconnected connections during HTTP request processing @wangfakang Fix read/write lock copy issue with dubbo registry @champly  ","excerpt":"We are happy to announce the release of MOSN v0.14.0. Congratulations to Changyu Yao (@trainyao) for …","ref":"https://brpc.incubator.apache.org/en/blog/releases/v0.14.0/","title":"Announcing MOSN v0.14.0"},{"body":"We are happy to announce the release of MOSN v0.13.0.\nNew Features  Support Strict DNS Cluster @dengqian Stream Filter @wangfakang that supports GZip processing Dubbo service registry complete Beta version @cch123 Stream Filter @NeGnail that supports standalone fault isolation Integrated Sentinel flow limiting capability @ansiz  Optimization  Optimize implementation of EDF LB and re-implement WRR LB using EDF @CodingSinger Configure to get ADMIN API optimizations, add features and environment variable related ADMIN API @nejisama Update that triggers a health check when updating Host changed from asynchronous mode to synchronous mode @nejisama Updated the Dubbo library to optimize the performance of Dubbo Decode @zonghaishang Optimize Metrics output in Prometheus, using regularity to filter out illegal Key @nejisama Optimize MOSN\u0026rsquo;s return status code @wangfakang  Bug fix  Fix concurrent conflict issues with health check registration callback functions @nejisama Fix the error where the configuration persistence function did not handle the null configuration correctly @nejisama Fix the problem that DUMP as a file fails when ClusterName/RouterName is too long @nejisama Fix the problem of not getting the XProtocol protocol correctly when getting it @wangfakang Fix the problem with fetching the wrong context when creating StreamFilter @wangfakang  ","excerpt":"We are happy to announce the release of MOSN v0.13.0.\nNew Features  Support Strict DNS Cluster …","ref":"https://brpc.incubator.apache.org/en/blog/releases/v0.13.0/","title":"Announcing MOSN v0.13.0"},{"body":"We are happy to announce the release of MOSN v0.12.0. Thanks to Sun Fuze (@peacocktrain) for his great contribution to this release, certified as commiter🎉 by MOSN Community Leaders.\nYou can see the changelog below.\nNew Features  Support Skywalking @arugal Stream Filter adds a new phase of Receive Filter execution, which allows you to execute Receive Filter @wangfakang again after MOSN has finished routing Host HTTP2 supports streaming @peacocktrain @taoyuanyuan FeatureGate adds interface KnownFeatures to output current FeatureGate status @nejisama Provide a protocol-transparent way to obtain requested resources (PATH, URI, ARG), with the definition of resources defined by each protocol itself @wangfakang New load balancing algorithm  Support for ActiveRequest LB @CodingSinger Support WRR LB @nejisama    Optimize  XProtocol protocol engine optimization @neverhook  Modifies the XProtocol heartbeat response interface to support the protocol\u0026rsquo;s heartbeat response to return more information Optimize connpool for heartbeat triggering, only heartbeats will be triggered if the protocol for heartbeats is implemented   Dubbo library dependency version updated from v1.5.0-rc1 to v1.5.0 @cch123 API Adjustments, HostInfo added health check related interface @wangfakang Optimize circuit breaking function @wangfakang Responsible for balanced selection logic simplification, Hosts of the same address multiplex the same health check mark @nejisama @cch123 Optimize HTTP building logic and improve HTTP building performance @wangfakang Log rotation logic triggered from writing logs, adjusted to timed trigger @nejisama Typo fixes @xujianhai666 @candyleer  Bug Fix  Fix the xDS parsing fault injection configuration error @champly Fix the request hold issue caused by the MOSN HTTP HEAD method @wangfakang Fix a problem with missing StatusCode mappings in the XProtocol engine @neverhook Fix the bug for DirectReponse triggering retries @taoyuanyuan  ","excerpt":"We are happy to announce the release of MOSN v0.12.0. Thanks to Sun Fuze (@peacocktrain) for his …","ref":"https://brpc.incubator.apache.org/en/blog/releases/v0.12.0/","title":"Announcing MOSN v0.12.0"},{"body":"We are happy to announce the release of MOSN v0.11.0. You can see the changelog below.\nNew features  Support the extension of Listener Filter, the transparent hijacking capability is implemented based on Listener Filter @wangfakang New Set method for variable mechanism @neverhook Added automatic retry and exception handling when SDS Client fails @pxzero Improve TraceLog and support injecting context @taoyuanyuan Added FeatureGate auto_config, when the feature is enabled, the dynamically updated configuration will be saved to the startup configuration @nejisama  Refactoring  Refactored XProtocol Engine and reimplemented SOFARPC protocol @neverhook  Removed SOFARPC Healthcheck filter and changed to xprotocol\u0026rsquo;s built-in heartbeat implementation Removed the original protocol conversion (protocol conv) support of the SOFARPC protocol, and added a new protocol conversion extension implementation capability based on stream filter xprotocol adds idle free and keepalive Protocol analysis and optimization   Modify the Encode method parameters of HTTP2 protocol @taoyuanyuan Streamlined LDS interface parameters @nejisama Modified the routing configuration model, abandoned connection_manager @nejisama  Optimization  Optimize Upstream dynamic domain name resolution mechanism @wangfakang Optimized TLS encapsulation, added error log, and modified timeout period in compatibility mode @nejisama Optimize timeout setting, use variable mechanism to set timeout @neverhook Dubbo parsing library dependency upgraded to 1.5.0 @cch123 Reference path migration script adds OS adaptation @taomaree  Bug fix  Fix the problem of losing query string during HTTP2 protocol forwarding @champly  ","excerpt":"We are happy to announce the release of MOSN v0.11.0. You can see the changelog below.\nNew features …","ref":"https://brpc.incubator.apache.org/en/blog/releases/v0.11.0/","title":"Announcing MOSN v0.11.0"},{"body":"Why use MOSN instead of Istio\u0026rsquo;s default data plane? Before the Service Mesh transformation, we have expected that as the next generation of Ant Group\u0026rsquo;s infrastructure, Meshization will inevitably bring revolutionary changes and evolution costs. We have a very ambitious blueprint: ready to integrate the original network and middleware various capabilities have been re-precipitated and polished to create a low-level platform for the next-generation architecture of the future, which will carry the responsibility of various service communications.\nThis is a long-term planning project that takes many years to build and meets the needs of the next five or even ten years, and cooperates to build a team that spans business, SRE, middleware, and infrastructure departments. We must have a network proxy forwarding plane with flexible expansion, high performance, and long-term evolution. Nginx and Envoy have a very long-term capacity accumulation and active community in the field of network agents. We have also borrowed from other excellent open source network agents such as Nginx and Envoy. At the same time, we have enhanced research and development efficiency and flexible expansion. Mesh transformation involves a large number of departments and R \u0026amp; D personnel. We must consider the landing cost of cross-team cooperation. Therefore, we have developed a new network proxy MOSN based on Go in the cloud-native scenario. For Go\u0026rsquo;s performance, we also did a full investigation and test in the early stage to meet the performance requirements of Ant Group\u0026rsquo;s services.\nAt the same time, we received a lot of feedback and needs from the end user community. Everyone has the same needs and thoughts. So we combined the actual situation of the community and ourselves to conduct the research and development of MOSN from the perspective of satisfying the community and users. We believe that the open source competition is mainly competition between standards and specifications. We need to make the most suitable implementation choice based on open source standards.\nWhat is the difference between MOSN and Envoy? What are the advantages of MOSN? Differences in language stacks\nMOSN is written in Go. Go has strong guarantees in terms of production efficiency and memory security. At the same time, Go has an extensive library ecosystem in the cloud-native era. The performance is acceptable and usable in the Service Mesh scenario. Therefore, MOSN has a lower intellectual cost for companies and individuals using languages such as Go and Java.\nDifferentiation of core competence\n MOSN supports a multi-protocol framework, and users can easily access private protocols with a unified routing framework; Multi-process plug-in mechanism, which can easily extend the plug-ins of independent MOSN processes through the plug-in framework, and do some other management, bypass and other functional module extensions; Transport layer national secret algorithm support with Chinese encryption compliance;  Is the open source MOSN the same version as the MOSN used internally by Ant Group? First of all, there is no so-called independent MOSN version inside Ant Group. Ant Group has many modules developed based on MOSN, and the internal modules rely on the open source version of MOSN. The research and development of business-independent MOSN core capabilities are carried out directly on the open source version.\nWhat is the difference between the open source and commercial versions of MOSN? Ant Group has commercial Mesh products. Commercial products mainly provide a complete solution from development to delivery runtime. At the same time, in order to meet the business needs of enterprise users, MOSN will be extended, so the so-called MOSN commercial version It mainly carries the version of the business user\u0026rsquo;s own business module.\nWhat is MOSN\u0026rsquo;s open source plan? The release cycle of MOSN open source is one month. We are about to announce Roadmap for 2020, and we look forward to co-building with more enterprises.\nWhat version of Istio does MOSN support? When will it be available in Istio? MOSN currently runs the bookinfo example based on Istio 1.5.2. In September 2020, Istio is expected to fully support the capabilities of Istio and become an integral part of Istio Sidecar deployment options available. Please join the MOSN community to learn about working with Istio.\nWhat service registration and discovery mechanisms does MOSN support? MOSN mainly supports two service registration and discovery mechanisms: one is to directly adapt to Istio, and the other is to integrate the SDK and use it with different registration centers and configuration centers.\nHow to participate in the MOSN open source community? Join the MOSN slack worksapce https://mosnproxy.slack.com to participate in the open source community. You can also visit the Community repository to learn about the organizational structure of the MOSN open source community and to obtain community materials.\n","excerpt":"Why use MOSN instead of Istio\u0026rsquo;s default data plane? Before the Service Mesh transformation, we …","ref":"https://brpc.incubator.apache.org/en/docs/faq/","title":"FAQ"},{"body":"We are happy to announce the release of MOSN v0.10.0. You can see the changelog below.\nNew features  Support multi-process plugin mode (#979, @taoyuanyuan) Startup parameters support service-meta parameters (#952, @trainyao) Supports abstract UDS mode to mount SDS socket  Refactoring  Separate some MOSN base library code into mosn.io/pkg package Separate MOSN interface definition to mosn.io/api package  Optimization  The log basic module is separated into mosn.io/pkg, and the log of MOSN is optimized Optimize FeatureGate (#927, @nejisama) Added processing when failed to get SDS configuration When CDS deletes a cluster dynamically, it will stop the health check corresponding to the cluster The callback function when SDS triggers certificate update adds certificate configuration as a parameter  Bug fixes  Fixed a memory leak issue when SOFARPC Oneway request failed Fixed the issue of 502 error when receiving non-standard HTTP response Fixed possible conflicts during DUMP configuration Fixed the error of Request and Response Size of TraceLog statistics Fixed write timeout failure due to concurrent write connections Fixed serialize bug Fixed the problem that the memory reuse buffer is too large when the connection is read, causing the memory consumption to be too high Optimize Dubbo related implementation in XProtocol  ","excerpt":"We are happy to announce the release of MOSN v0.10.0. You can see the changelog below.\nNew features …","ref":"https://brpc.incubator.apache.org/en/blog/releases/v0.10.0/","title":"Announcing MOSN v0.10.0"},{"body":"","excerpt":"","ref":"https://brpc.incubator.apache.org/en/index.json","title":""},{"body":"This is the blog section. It has two categories: News and Releases.\nFiles in these directories will be listed in reverse chronological order.\n","excerpt":"This is the blog section. It has two categories: News and Releases.\nFiles in these directories will …","ref":"https://brpc.incubator.apache.org/en/blog/","title":"Blog"},{"body":"  #td-cover-block-0 { background-image: url(/en/featured-background_huac82deb8a78a73a5e43d2f17f70ceea6_15294_960x540_fill_q75_catmullrom_top.jpg); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/en/featured-background_huac82deb8a78a73a5e43d2f17f70ceea6_15294_1920x1080_fill_q75_catmullrom_top.jpg); } }  An industrial-grade RPC framework for building reliable and high-performance services. Learn More   Get started! C++  Java           Why bRPC?  bRPC is an open source industrial-grade RPC framework used throughout Baidu, with 1,000,000+ instances(not counting clients) and thousands kinds of services. You can use it to:  Build a server that can talk in multiple protocols (on same port), or access all sorts of services.  restful http/https, h2/gRPC. using http/h2 in brpc is much more friendly than libcurl. Access protobuf-based protocols with HTTP/h2+json, probably from another language.  redis and memcached, thread-safe, more friendly and performant than the official clients. rtmp/flv/hls, for building streaming services. hadoop_rpc (may be opensourced) rdma support (will be opensourced) thrift support, thread-safe, more friendly and performant than the official clients. all sorts of protocols used in Baidu: baidu_std, streaming_rpc, hulu_pbrpc, sofa_pbrpc, nova_pbrpc, public_pbrpc, ubrpc and nshead-based ones. Build HA distributed services using an industrial-grade implementation of RAFT consensus algorithm which is opensourced at braft  Servers can handle requests synchronously or asynchronously. Clients can access servers synchronously, asynchronously, semi-synchronously, or use combo channels to simplify sharded or parallel accesses declaratively. Debug services via http, and run cpu, heap and contention profilers. Get better latency and throughput. Extend brpc with the protocols used in your organization quickly, or customize components, including naming services (dns, zk, etcd), load balancers (rr, random, consistent hashing)   Used by Providing your info on Wanted: who’s using bRPC to help improving bRPC better                       bRPC is an Apache incubation project.    This is the second Section     -- ","excerpt":"#td-cover-block-0 { background-image: …","ref":"https://brpc.incubator.apache.org/en/","title":"bRPC"},{"body":"MOSN is an open source project that anyone in the community can use, improve, and enjoy. It was open-sourced by Ant Group on July, 2018. We\u0026rsquo;d love you to join us! Here\u0026rsquo;s a few ways to find out what\u0026rsquo;s happening and get involved.\nCommunity materials For more materials about MOSN community, please go to the community repo.\nWorking groups MOSN including the following working groups now.\n Istio Working Group Dubbo Working Group  Choose to join a working group that interests you and start your MOSN journey!\nCommunity meeting MOSN community holds regular meetings.\n  Wednesday 8:00 PM CST(Beijing) every other week\n  Meeting notes\n  Partners Partners participate in MOSN co-development to make MOSN better.\n                      End Users The MOSN users.\n                                Please leave a comment here to tell us your scenario to make MOSN better!\nEcosystem The MOSN community actively embraces the open source ecosystem and has established good relationships with the following open source communities.\n                      Committers The MOSN Community Certified Committer is as follows.\n   Name GitHub 公司     Yang Tian taoyuanyuan Ant Group   Fakang Wang wangfakang Ant Group   Peng Bai nejisama Ant Group   Chunhui Cao cch123 Ant Group   Fuze Sun peacocktrain zhipin.com   Peng Chen champly dmall.com   Changyu Yao trainyao youmi.net   Qian Deng dengqian Alibaba Cloud   Runhao Huang glyasai goodrain   Zechao Zheng CodingSinger ByteDance    Committer is an individual with MOSN repository write access to the following criteria.\n Individuals who can make sustained contributions to issues, PR over time. Participate in the maintenance of issue lists and discussion of important features. Active participation in code reviews and community meetings.  Meetup https://github.com/mosn/meetup\nTutorials See MOSN tutorials.\nCommunication Communicate with MOSN developers on Slack workspace.\n","excerpt":"MOSN is an open source project that anyone in the community can use, improve, and enjoy. It was …","ref":"https://brpc.incubator.apache.org/en/community/","title":"Community"},{"body":"","excerpt":"","ref":"https://brpc.incubator.apache.org/en/search/","title":"Search Results"}]